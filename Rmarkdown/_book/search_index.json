[
["index.html", "Preface", " Preface "],
["introduction.html", "1 Introduction 1.1 The Scenarios 1.2 Some solutions", " 1 Introduction Welcome to the Programmer Knowledge Base. Over the last few years, I’ve noticed that young data scientists are often very statistically advanced, but are sometimes programmatically beginners. The goal of this book is to introduce some best programming practices to produce high-quality code. The best practices are first introduced as concept and then are exemplied for several different languages. 1.1 The Scenarios Imagine that you worked on a project a year, or even several months ago. While you were working on that project you knew everything about it, the ins and outs and all the fine print. Now several weeks, months, or maybe even a year has passed and someone comes to you to ask how you generated the result. You go back to the code to look it up and give them some more details, but then you remember that you left the project folder a little messier than intended and never had to the time to go back and clean it up. In fact, it was a mad scramble until the end, so things are definitely messy. As you’re looking over the output, you try to recall if you turned in final_final_final_v2 or final_final_v3 report. And which code created that output, create_df1_final or create_df2.2? But wasn’t there a bug in create_df2.2 that you found right before the output was due, so you actually used create_df1_final_JH? But maybe another programmer worked on that part with you, so did you use their version or yours? Wow, you wish you had updated the documentation to tell you how to run the program in the first place. Because you vagually recall there was some other script you had to run befor eyou could run create_df1_final_JH. And once you think you’ve maybe figured out which code was used to create which output, you realize all the packages you used have been updated since, so some of the functions you wrote have stopped working correctly. Or maybe this is the scenario. Another programmer left the company a week ago and you’re responsible for picking up the pieces. You open the folder. There’s a word document with a few vague guidelines about how the program is supposed to run that was updated 3 months ago, when the code as updated 1 month ago. Scripts are poorly named, and you’re really unsure where to start. You open the first script that says main and it’s 4000 lines long. There are some functions in that script, but none of the functions have any documentation. Well this is going to be fun you think as you see for-loops nested in for-loops nested in for-loops … Or maybe it’s this scenario. You’re trying to re-create summarized tables from a code base. The numbers just seem a little off and you want to verify the results make sense. But the code wasn’t version controlled, and the data that were used to create the aggregate tables were overwritten several times because the name was hardcoded and there wasn’t a date or anything else that changed. Any attempt of tracing back the original code to create the original output is gone. Or maybe this scenario. You shipped the output to the client/partner. Hallelujah! You reviewed the output and your co-workers did too. Several weeks later your recoding something and you notice some of the numbers seem really off, like magnitudes different than the output your sent before. Oh no, you’ve discovered the bug in the code after it gets shipped to the client/partner. Or maybe you discover just before you ship the output to the client/partner, but it’s a mad scramble and everyone has to re-run their individual piece of the pipeline. Yikes!! STOPPPPPP! I think a lot of folks working on teams with data have come across at least one, if not multiple of these scenarios. Even teams that really stress QA/QC can find themselves in these situations. They’re uncomfortable, stressful, and often result in some later work nights than intended. I have personally come across all of these and contributed to several when I didn’t have guidance on how to do things better. Thankfully a lot of smart people, who have spent a longer time than myself thinking about and dealing with these issues, have come up with some great solutions! 1.2 Some solutions The simple solution is this: lets build robust high-quality code. What do we mean by ‘robust high-quality code’? In my mind, there are several parts to robust high-quality code. High-quality code a) is reproducible across three dimensions: people, time, and machines and b) efficient c) tested. Reproducibility “There’s always at least two people on a project, you and future you”. People: code should be reproducible across multiple programmers. Programmer B should be able to run Programmer A’s output and return the same output. Time: code should be reproducible across time. Code run today should still be able to be run a year from now and return the same output. Software/Machines: Code should not be machine specific! This is an especially important concept for those using open-source software. Open-source software can change extremely quickly, and functions or packages that were used a month ago, may no longer work in the same way or have the same functions today. There’s no one way to maintain reproducible code bases across these three dimensions, but there are several best practices that the broader data science, statistics, and computer science communities seem be in consensus on. Specificially we’ll talk about1: Configuration files Documentation Code version control and the software development lifecycle Virtual environments and package managers Building packages Data Archiving Repetition vs. Replication vs. Reproduction Repetition Replication Reproduction Same Lab Different Lab Different Lab Same Methods Same Methods Different Methods Quality Testing is an integral part to being a good programmer, whether you’re building software for an application or building software for data analysis, it is integral to test code. Specifically we’ll talk about: Code Review Assertions Unit testing Integration testing Efficiency Specifically we’ll talk about: Benchmarking Profiling Parallel processing Computing resources (Azure, AWS, etc.) note: these topics are not listed in order of importance, they are all very important!↩ "],
["reproducibility-1.html", "2 Reproducibility 2.1 Configuration files 2.2 Documentation 2.3 Git Version Control 2.4 Environments", " 2 Reproducibility 2.1 Configuration files One thing that programmers constantly face is a changing set of parameters that are used as inputs to any script. Whether this is new data, or a new title for a graph, it always seems to change. If these parameters are hardcoded into a script, the likelihood that you’ll be able to remember where all those inputs are is low. Also, it can make debugging more challenging! An easy way to avoid these issues is to use a configuration file. Introduction A configuration file is a file which contains all of our parameters that are likely to change of the lifecycle of the project. Things like paths, variable names, visualization titles, and captions are best stored in a configuration file and then loaded at the top of the script where they are used. Configuration files are also important to version control. Say that you changed the title of a visualization code, but not the actual function that creates the visualization. If title is hardcoded into the script, then when you go submit a pull request the other programmer reviewing the pull request will have to double-check that it’s not the function that change, but just the text for the title. If that title were stored in a configuration file, and let’s say you made many changes to many titles, then the other programmer only has to review the one file instead of a bunch, and the underlying code base stays unchanged. Using a config file, also makes it less likely that bugs will occur. If you path is embedded down in line 403, and you forget to update it to the most recent datafile, uh oh, wrong data gets used or produced! Formats Different programmers have different configuration file preferences. The common ones are YAML, JSON, and CSV. My personal preference is a YAML file and that’s because you can add comments, which you can’t do with JSON, and you can create a nested format, which you can’t do with a CSV. The nested format makes it easy to loop through different models or dataframes which are going to utilize the same functions. YAML ## You can add comments to YAML! VAR1: 1 VAR2: 2 Some YAML notes: If using nested yaml structure, make sure to indent with spaces, not tabs! Add a final line to a yaml config file, or some software will have difficulty reading it in JSON { &quot;VAR1&quot; = 1, &quot;VAR2&quot; = 2 } Some JSON notes: JSON can’t do comments Web-developers will often use a JSON file because JSON stands for JavaScript Object Notation CSV VAR1 VAR2 1 2 CSV notes: CSV files can be really handy ways to organize script parameters, especially if you’re working on a team with people who aren’t used to working in a JSON or YAML file One limitation of a CSV is their flat structure, it’s often really nice to be able to nest attributes within other sections of a config file 2.2 Documentation Documentation is an important step to making code reproducible. Document your code and systems for yourself, other programmers/team members, and maybe most importantly: your future self. It’s unlikely you’ll remember the exact steps you took to produce some output a year from now, but that’s what documentation is for! Repositories Repositories are documented with a README.md file and stored in repository root directory. my_repo\\ README.md The .md suffix indicates a markdown file. If you have used markdown before here are some resources: Example README Markdown syntax Markdown table formatting Udacity Course on READMEs Markdown is an important skill to learn. Many technical documentation systems are created using markdown because it’s easy to learn and can be version controlled with ease unlike other word processing software (e.g. Microsoft Word, or Google Docs). README READMEs are documentation for any humans who will need to use that code, including: Yourself Your future self Co-workers or other people who will contribute to your code People who will use your code Clients if the code will be delivered to them at the end of the project (some code technically belongs to clients) On any version control system (e.g. Github, Bitbucket, TFS) the README will be rendered as HTML and displayed on the repository page. General README Structure In general, READMEs should follow this format, but they of course can deviate to provide more or less information where necessary. A paragraph describing the high-level purpose of the package An example showing how to use the package or repository of code to solve a simple solution Installation instructions An overview that describes the main components of the package General questions to answer in a README: What are the exact steps that need to be taken to get up and running? What should someone already have installed or configured? What might someone have a hard time understanding right away? Any known issues or bugs? Where are the data for this analysis stored? What do we expect those data to look like? Code Functions Always document your functions. Include a short description about what the function does, what parameters the function takes, what the function returns, and even a short example! If you find yourself have trouble summarizing what the function does in a short description, this is a good indicator that you need to break that function into multiple parts. This is also very important for testing, which we’ll get to later on! Code-specific documentation best practices will be discussed in the 03-languages section. Commenting Code One of the first things many programmers learn is to comment their code, to provide information about the steps they are taking. While we want to encourage commenting code, we also want to be careful. Some programmers will write huge blocks of text before executing a number of steps. If this is you, we want to make things more modular. One line should be able to describe any function. Data Data Dictionaries Like code, it’s important to document your data with a data dictionary. A data dictionary contains information about variable names and their corresponding meta data. A data dictionary should always contain labels, as variables often aren’t easy to identify what data they represent. A data dictionary can also contain information such as: variable type (e.g. numeric, string, date, or integer, float if you want to get more specific), variable length, allowable values for descriptive variables, units of measure for numeric values. Data Dictionary Resources: Best Practices Systems One of the more challenging things to document is a system. Generally speaking, if we’re talking about a system, then we’re talking about a team of people responsible for developing and maintaing a code base, as well as other documentation. If possible, it’s best to store code repositories at the project level. So all of the repositories that contribute to a system are stored under the same project. Then at the system-level create a README to document how the system works. General questions to answer: * How the output from one repository used as an input in another repository? * What tests are run in-between? * What are the necessary parameters for each section of a system to run? * What is the timeline of the system run? Hours, Days, Months? 2.3 Git Version Control Introduction Version control systems allow for easy access to prior versions of code, branching for development, and easy of collaboration between programmers. Different version control systems exist across several industries, but it is considered best practice to use a git as your version control system for any coding task. Git is important to use to keep track of code version histories. Code isn’t static and just written once and executed, but is consistently changing. Version control is necessary to keep track of these updates. Resources: Udacity Git Class https://datasift.github.io/gitflow/IntroducingGitFlow.html https://git-scm.com/book/en/v2 https://homes.cs.washington.edu/~mernst/advice/version-control.html https://github.com/linusmarco/git-training/blob/master/src/markdown/slides.md Repository set-up Each repository should start with these branches master All code in this repository has been tested and is solid This code has been ‘released’ develop The develop branch is created from the master branch This code is has also been tested and is solid but is pre-release feature branches Feature branches are stored in the features/ folder Feature branches are created from the develop branch This code is in development It may be pre-tested right now and is still in progress Glossary Term Definition B branch An active line of development. More literally it is a local copy of the code. Branching off of the master branch preserves the stability of the code. bitbucket An Atlassian another server used to host code C checkout The action of updating a branch clone The action of copying the repository hosted on server to a local computer commit A single point in the code’s history. It contains a hash #, which is unique and a commit message which indicates why changes were made to the code base. F fetch To get the branch’s head from the remote repository G git Version control system a tool to manage your source code history github A public server to host vresion controlled code gitignore A file used to indicate which file types should not be tracked by git L local The version of the code that exists on your computer; your local repository M master The default development branch! You can think of master as the stable version of your code. If you need to make updates/changes to the code, consider making those changes on a branch and then merging that branch into master once your code has been code reviewed and tested for bugs. merge To bring the contents of a different branch into the current branch. O origin The upstream (original) repository. P pull To fetch a branch and merge it. push To send the changes from the local branch to the remote branch. R remote The version of the code that exists on the server; i.e. the remote repository. repository (repo) A collection of code, commits, branches, and tags. S stage Code must be staged before it can be committed. T tag A reference typically used to mark a particular point in the commit chair. U unstage V VIM WorkFlow Download the code base Download the code base from the remote repository. This is called pulling. Navigate to the folder where the code base should be stored by using the cd command. git clone path_to_repo Checkout a branch Checking out a branch is like opening a file. A branch is a group of files that are the “Saved As” version of the files on the master or develop branches. git checkout branch_name Make changes to the code base Edit your code. Remember to test the code before commiting it. Try to avoid commiting broken code! Review which files changed We always want to review which files changed, to make sure we don’t miss any changes that other code relies upon. git status Review changed files Review the changes. Imagine you had track changes on in Word and you can see the old and new versions. git diff file_name Stage the file(s) for commitment git add file_name Write a commit message Make the commit message short, and descriptive of the why, not the what! Remember that commits should be frequent enough that “and” isn’t needed in the commit message git commit -m &quot;commit message&quot;. Update remote repository Repeat all prior steps until you’re ready to update the code on the remote repository. git push origin branch_name Pull Request Get another programmer(s) to review the code by submitting a code request to the develop or master branch. TODO: Add more information about PULL REQUESTS Checkout changed code Once the pull request has been accepted pull down the updated code from remote. git checkout other_branch git pull other_branch Visualizing Git Flow (from Atlassian) Image from Atlassian In-depth development workflow A develop branch is created from master Feature branches are created from develop When a feature is complete a pull request is made for a potential merge into the develop branch This request goes through a code review and can be approved/rejected by the technical lead When develop branch is complete for a given cycle, a release branch is created from develop When the release branch is tested and deemed ready for production it is merged into develop and master If an issue in master is detected a hotfix branch is created from master Once the hotfix is complete a pull request is made for a potential merge to both the develop and master. Resources Atlassian Gitflow The Dos and the Don’ts Git Don’ts Version control sensitive data This is a security concern. Servers which host remote repositories are not necessarily as secure as the network the data are stored on. Additionally, storing the data in more than one location increases the likelyhood that a data breach could occur. Use git as a data versioning tool Git is not a data versioning tool. This book will discuss data versioning at a later point! Storing data on the git (even unsensitive data) will clutter the repository’s history. An exception to this rule may be storing static metadata (e.g. config file) or fake data used for unit testing, along with some other well thought-out special cases. Think very carefully about whether there’s a good reason to put data in the repository and seek guidance before doing so. Version control binary files Any file that cannot be opened by a text editor (Notepad, Atom, Sublime, etc.). If you are unsure about whether a file is a binary file, just try to open it in Notepad—if you see a bunch of gibberish then it is! If you see a proper text representation of what you know the file contains, then it isn’t! Git is a version control tool, meaning that it tracks changes to the contents of a file. This works great for text files, but not at all for binary files. If you change a single character of a Word document, all of the contents of the binary file will change. This means that git will not be able to show you what changed about the file, only that the file changed. Don’t version these file types: Word documents (.doc, .docx) Excel files (.xls, .xslsx, .xlsm, .xlsb, …) Images (.jpg, .png, …) Executables (.exe, .dll, …) SAS/Stata/other data files (.sas7bdat, .dta, .dat (sometimes), …) There can be exceptions, but generally it is bad practice to version control this type of file. They, like data, will bloat the repository for no good reason. Version control temporary files Windows and other programs often create temporary files when a file is in the process of being written or when a team member has the file open. They often have weird names starting with “~$” or extensions like “.bak”. Don’t version control these as they are temporary! Version control output files This includes log files and other direct program output. Like binary files, these files are likely to change a lot when they change, but they are also conceptually more like data than they are like code. Use name versioning for files tracked by git Code should never be names _v1 or _v2_final or _v3_final_final_JH. Git takes care versioning for you! The days of archiving code and renaming different versions are over! Just make your change and commit it. Then, to access the previous version, just use git checkout or use git diff or git show to view changes. Gits Dos Write concise and informative commit messages Commit messages are the easiest way for other collaborators and future versions of yourself to figure out what changes were made in each commit. This makes it easier to trace errors, keep track of features, and find explanations for changes. Remember the commit message should explain the why not the what. Make each commit a single cohesive change Conceptually, each commit should be a single unit of work (e.g. “fixed bug X” or “added X section to report”&quot; or “changed working in section X”). This unit of work could be small (a couple lines of code, or even fixing a typo) or it could be large (adding an entire section to a program), but all changes/additions/deletions within a commit should be related. A quick rule is that you should rarely need to use the word “and” in a commit message—if you are, then you should probably make that commit into two or more commits. Doing this helps everyone get a better sense of what each commit changed, but also makes it easier to inspect and roll back changes. We wouldn’t want to have to undo a change unrelated to the one that actually needs to be reversed When adding files to be committed, use git add [file/s] to add only certain modified files to staging. Use git reset to unstage all files if you accidentally add too many Review all changes before committing This quick and simple step should make sure that you avoid all of the “do nots” above 99% of the time. First add the files you want to commit: git add [files] or git add . to stage all files for commit Review the changes: git status to see a list of all files that will be committed, and git diff --cached to see a detailed view of all changes to each staged file Commit! git commit -m [my concise and informative commit message] Use a .gitignore file A .gitignore file that tells git which file types not to track. Git will ignore any files that match patterns specified in the .gitignore. Gitignore A .gitignore file tells git which files should not be tracked by version control. Common files extensions to inlucde data file extensions, binary files, executables etc. If a .gitignore is included in the working directory the command git add . will only add files where the file extensions do NOT appear in the gitignore. Create a .gitignore In the command line/terminal type echo &gt; .gitignore To create the file manually, save the files and folders that start with a period with a trailing period. E.g .gitignore. Where to put the .gitignore Generally, you want the file location to be at the root directory of the project, in the same directory as the .git folder Extensions often included in a .gitignore Data *.csv *.xlsx *.docx R *.Rdata *.RDS Python Good python suggestions SAS *.sas7bdat Stata *.dta Git still tracking files marked in gitignore This happens because these files were originally add to git. Even if the files are removed from git and added into the .gitignore, git still wants to track the files. Here’s the stack solution. Common Commands Action Git Command Branching (like a Save As for code) git branch new_branch_name To pull down a local version of a remote branch and switch to that branch git checkout --track origin/remote_branch_name To fetch all branches git fetch --all To rollback to an old Git commit on the repo. Revision is the commit hash. git checkout [revision] . To create and checkout a branch git checkout -b branch_name To undo the rollback to an old Git commit on the repo git reset --hard To check which tracked files have been changed git status Compare code changes between previous commit and current code git diff name_of_file To stage a modified file to be committed git add name_of_file To un-add a modified file git reset HEAD To commit staged files to your local repo without opening VIM to write a commit message git commit -m 'commit message' To push your local repo to the remote repository git push origin local_branch_name Amend or change a commit message git commit --amend To push the new branch to the remote git push --set-upstream origin &lt;new_branch_name&gt; Delete a branch on remote (post-merge w/ master and post-code review!) git push --delete &lt;remote_name&gt; &lt;branch_name&gt; Delete a branch locally (post-merge w/ master and post-code review!) git branch -d &lt;branch_name&gt; VIM Editor The VIM (Vi IMproved) editor is a text editor for unix/linux operating systems. However, sometimes when using git commands the editor will appear. Examples of when it appears are during merges, or editing commit messages. One of the only commands you need to know with the VIM editor is how to save and close. To do so: hit the Esc button on the keyboard. And then type :wq to save and exit. Additional Resources: save and close cheatsheet 2.4 Environments Note this section is only application to users of open-source software. SAS and Stata users, you can skip this section, unless of course you’re interested in learning more! Introduction The cool thing about open-source software is that anyone with a knowledge of that software can help build packages for the software community. Open source software is amazing, because there’s a huge community of programmers out there trying to achieve similar goals given similar constraints. There are however, some big downsides to utilizing open source software. One of the biggest is that developers of packages churn out updates that may or may not be backwards compatible with older versions of their packages. packages are rolled-out continuously and aren’t built as part of one central, commercialized system, frequently resulting in version control issues for software users. And it goes beyond packages! Python rolled out version 3 of the language in 2008.2 Yet, a lot of Stack Overflow answers are still written using Python 2. When I try to run Python 2, I get errors, which then have to be debugged.3 Fortunately, there’s a simple solution: multiple development environments! Using Multiple Environments Code that doesn’t break Ease of collaboration Client deliverables Read this article to learn more! https://learntocodewith.me/programming/python/python-2-vs-python-3/#history-of-python2-vs-3↩ 99% of the time this is related to the print statement and takes 2 seconds to change, but the example of why this is an issue remains!↩ "],
["quality-1.html", "3 Quality 3.1 Testing 3.2 Code Review", " 3 Quality 3.1 Testing Mistakes happen and bugs are introduced. It’s something that’s unavoidable, because we’re human. However, we do want to catch those miskate to ensure high-quality code. Testing is one way to automate the discovery of such mistakes. While it’s challenging to prevent any mistakes from occurring, it’s important to shift the discovery of these issues from later to earlier. Testing code results in: Fewer bugs Testing makes it explicit how the code should behave Better code structure Code that is easy to test is designed better Testing forces the programmer to break up complicated parts of the code into separate parts that work in isolation Easier restarts and collaboration Easier to pick-up where you left off if you know what’s passing/failing Other programmers can figure out the goals of the code based on the tests and feel more confident making changes and re-running the code because the tests are passing Robust code Adds confidence that you know what your code is doing Best practices Tests should be independent, chill, implementaiton agnostic, fast, and shareable. Term Definition Independent Tests shouldn’t depend on other tests, or auto-fail when other tests fail. This makes is hard to tell what code needs to be worked on. Chill Tests shouldn’t be super strict on timing or require super precise output. Very relaxed tests are often still quite good. Implementation agnostic Tests shouldn’t assume implementation details and test those details. Fast These tests are meant to be run constantly, many times per day. Longer end to end testing should be done as a separate step. Shareable Don’t test with any sensitive data. You want to be able to communicate the results of your test without risk. Testing timeline Testing is often one of those things that is thought of last not first, often after a bug has been discovered! A better strategy is to build testing time into the development timeline. And test as you program, don’t leave it all until the end. Testing as you go along also helps to re-organize code in a more efficient way. You would never submit a paper or report after the first draft. Code shouldn’t be submitted after the first draft either! It needs to be edited and thoughtfully reorganized throughout the development process and testing can help encourage this practice. Types of tests Assertions An assertion is statement that evaluates to true or false. If the statement evaluates to true then the assertion passes. If the statement evaluates to false then it throws an assertion error. Assertions are the building blocks for unit tests and integration tests. At minimum, code should have strategically placed assertions throughout. However, it is much preferred to have an organized set of test files Unit Tests A unit test is a test written by the programmer to verify that a relatively small piece of code is doing what it is intended to do. They are narrow in scope, they should be easy to write and execute, and their effectiveness depends on what the programmer considers to be useful. The tests are intended for the use of the programmer, they are not directly useful to anybody else, though, if they do their job, testers and users downstream should benefit from seeing fewer bugs. Part of being a unit test is the implication that things outside the code under test are mocked or stubbed out. Unit tests shouldn’t have dependencies on outside systems. They test internal consistency as opposed to proving that they play nicely with some outside system. Definition from Stack Overflow Integration Tests An integration test is done to demonstrate that different pieces of the system work together. Integration tests cover whole applications, and they require much more effort to put together. They usually require resources like database instances and hardware to be allocated for them. The integration tests do a more convincing job of demonstrating the system works (especially to non-programmers) than a set of unit tests can, at least to the extent the integration test environment resembles production. Actually “integration test” gets used for a wide variety of things, from full-on system tests against an environment made to resemble production to any test that uses a resource (like a database or queue) that isn’t mocked out. Definition from Stack Overflow Data validation tests Some things to test for: Column types Column names Null or NA values Boolean values Values within a certain range Should character vectors be a certain length Do we expect a certain range of values for a given categorical variable 3.2 Code Review A code review is the chance to walk-through and explain your code line-by-line to others involved with the project. Code reviews provide opportunities for other programmers or researchers to help catch bugs and improve overall code quality. Review participants The original author(s) of the code being updated At least one more senior or programmer of an equivalent level People knowledgeable about the content area of your code At least one person who is very familiar the programming language General tips for success Before: If there are data involved and you wish to run the code line-by-line, pre-load the data (or at least a sample of the data) Comment the code as this will make it easier to walk through or explain things on the spot Make sure all the functions have documentation Show that all the tests are passing Modularize! Make functions, have multiple scripts, write a package. This will make it easier to walk through. During: Take notes in the code when your reviewers give feedback, or ask one of the other people involved in the review to take notes for you. After: Ask for clarification, especially if you’re new to the project or software you’re using. More experienced programmers know what they’re doing, but they may be unaware of your confidence in a language/project Once you’ve incorporated the feedback, make sure to ask someone to review the changes "],
["languages.html", "4 Languages 4.1 Python 4.2 R", " 4 Languages 4.1 Python Introduction Python is primary used for web testing, pipelining, data management, data science, and visualization. Installation Download Python 3.7.0 from Anaconda Open Bash and check that Python is saved to the AppData folder where python should return something like C:\\Users\\jh111\\AppData\\Local\\Continuum\\anaconda3\\python.exe where pip should return something like C:\\Users\\jh111\\AppData\\Local\\Continuum\\anaconda3\\Scripts\\pip.exe If either of these throw an error, like “could not find files…”, python and pip need to be added to the PATH environment variable Anaconda Background We use the Anaconda distribution of python. There are several advantages to using Anaconda over base python, including: An Anaconda installation comes with pre-built binaries for many popular packages, including several that we use frequently (numpy, pandas). Installing some of these (e.g. numpy) without Anaconda is very tricky and requires administrator permissions. Anaconda comes with very nice environment management tools, which not only allow you to manage your package dependencies per-project, but also your python version. Data Management Modeling Visualization Web Reproducibility Quality Efficiency 4.2 R Introduction R is used primarily for data cleaning and modeling. R is also used to build simple applications and html files using Rmarkdown, Rshiny and Rshiny dashboards. R can also be used with for Bayesian analyses. Installation Download R 3.5.2 Download Rstudio4 Download Rtools35 Package Repository R’s main package repository is CRAN CRAN CRAN (the Comprehensive R Archive Network) is a “network of ftp and web servers around the world that store identical, up-to-date, versions of code and documentation for R.” Every package installed from CRAN has met the CRAN Repository Policy, been accepted into CRAN, and is currently housed there. Once a package is on CRAN, it is freely available for anyone in the world to install and use. Initial Workflow Follow these initial first steps to set-up your R-project. Start a new project Start a new project or package. Hadley shows how to do this here in section 8.4 RStudio projects. The main reason to use an R project is to help keep things organized. Programmers can set-up functionalities that are specific to that project, but not others. One example is that a programmer can set-up certain packages to always be loaded for that project. Using R Projects also allows programmers to use relative paths. Initiate the project with packrat Packrat is a package manager for R. E.g. packrat::init(infer.dependencies = FALSE). This will create a packrat folder in the project directory Create folders/files R Projects Create Folders Create Files R projects R/, tests/ .gitignore, config.yaml, README.md R packages tests/ .gitignore, LICENSE, README.md Make a folder: mkdir R Make a file: echo &gt; config.yaml Install packages Packages will be installed into the project’s library not the global R package library, so for each project the packages will be re-install the packages. This done intentionally, so that different packages may depend on different versions of a package. Install these packages: devtools assertthat testthat Suggested install: tidyverse roxygen2 formatR Set-up the testing space Run use_this::use_testthat() to set-up the testing space Version control files .gitignore .Rprofile config.yaml README.md XXX.Rproj packrat/ init.R packrat.lock packrat.opts src R/ tests/ If applicable: DESCRIPTION LICENSE NAMESPACE .Rbuildignore man/ Folder structure At this point the folder structure should look like this: project_dir/ packrat/ R/ tests/ .gitignore .Rprofile config.yaml README.md XXX.Rproj Style Guide General Rules Variable names and function names should be lowercase or camel case (be consistent) Avoid using names of existing functions/variables Spacing Put spaces around all infix operators (=, +, -, &lt;-, etc.) ALWAYS put a space after a comma, and never before Put a space before left parentheses, except in a function call Curly Braces Opening curly brace { NEVER goes on its own line, but should always be followed by a new line ALWAYS indent code inside curly braces Closing curly brace } always goes on its own line, unless followed by an else Line Length Roughly 80 - 90 characters per line Indentation ALWAYS use two spaces NEVER mix tabs/spaces Assignment ALWAYS &lt;- not = Comments Comments should begin with # Comments should explain the why not the what Comments that are too lengthy indicate a function should probably be used to break up the code chunk Other Style Guides Google’s style guide Styling Packages formatR Automatically formats and spaces code for more human-readability. library(formatR) formatR::tidy_dir(&quot;R&quot;) styler Goal is to provide non-invasive pretty-printing of R source code while adhering to the tidyverse formatting rules. library(styler) ugly_code &lt;- &quot;a=function( x){1+1} style_text(ugly_code) # style_file() # style_dir() # style_pkg() lintr Warns you about styling errors and potential problems. library(lintr) lintr::lint(test.R) Debugging R offers a number of useful built-in functions for debugging, and user-contributed packages are also available to provide additional functionality. The simplest method is to insert commands into your code to print diagnostic“check points” during its execution, using functions such as cat(), print(), or sink(). R also has a more sophisticated debugging system, which is supported by a GUI in the RStudio IDE. The most direct way to mark a function for debugging is to use debug(functionname). When this function is called, normal execution will be suspended, and you will enter the browser display. Commands within the debugged function can then be executed one at a time, and output can be inspected by the programmer along the way. You can set a function to begin debugging partway through its execution with setBreakpoint() or trace(). If an error is encountered (whether or not the function was being debugged), the traceback() command can be used to view the stack of nestedfunction calls at the time of the most recent error. We illustrate debugging in RStudio with the following example. Suppose we want to debug the lm() function to see how it works inside: debug(lm) # mark lm() for debugging lm(speed ~ dist(data=cars)) A few things happen when you execute the debugged lm() function: In the Source pane, a window called the Source Viewer opens. This contains the code of lm(). At the bottom of the Environment pane, the current stack of function calls is listed under Traceback. The console enters Browser mode. In browser mode, a set of buttons appears along the top of the console (including Next, Continue, Stop, and two arrow buttons), and the console prompt becomes Browse[2]&gt;. The current location in lm() is indicated by the green arrow on the left side of the Source Viewer. In the above figure, the green arrow is on line 4, after the function declaration, but before any of the commands within the function have been executed. This means that all the variables that were entered (or defaulted) in the function call are available. We can enter commands at the command prompt as usual, and they will be executed in the environment of the function: The commands of the lm() function are executed by clicking the button, by typing n+Enter, or by pressing Enter in the console. Any of these actions will execute the next command in the lm() code, and advance the green arrow in the Source Viewer to the next command. The other control options are: At any point in executing the lm() code, you can run your own commands to look at the contents of an object created within lm(), for example, or check objects for errors. When you are finished debugging, exit debugging mode and then run undebug(lm), so the function can be used normally without debugging. More information on debugging in RStudio can be found here, or in the chapter on debugging in the Advanced R book. External packages such as debug and edtdbg contain alternative debugging interfaces, which are not described here. Data Management Packages Package Description and Usage dplyr Tools for manipulating data frame reshape2 Tools to reshape data frames tidyr Tools for creating “tidy” datasets data.table Enhanced data.frame for faster operations lubridate Tools to parse and manipulate dates stringr Tools to manipulate character strings Matrix Sparse and dense matrix classes abind Combines multi-dimenstional arrays Modeling 4.2.0.1 Packages Package Description and Usage survey Analysis of complex survey samples twang Toolkit for weighting and analysis of nonequivalent groups Matching Multivariate and propensity score matching with balance forecast Forecasting functions for time series and linear models boot Functions for bootstrap analysis multcomp Tools for multiple comparison testing rstan R interface to Stan for Bayesian analysis rjags Just another Gibbs sampler in R survival Survival analysis nnet Neural networks and multinomial log-linear models lme4 Linear and non-linear mixed effects models nlme Linear and non-linear mixed effects models mgcv Generalized additive models refund Regression with functional data spatial Functions for kriging and point pattern analysis KernSmooth Functions for kernel smoothing lars Least angle regression, lasso, and forward stagewise glmnet Lasso and elastic-net regression methods caret Classification and regression training rpart Recursive partitioning for classification and regression trees randomForest Random forest methods from machine learning cluster Cluster analysis Visualization Web Reproducibility Config files YAML ## You can add comments to YAML! VAR1: 1 VAR2: 2 Read in library(yaml) config &lt;- yaml.load_file(&quot;path_to_config&quot;) var1 &lt;- config$VAR1 var2 &lt;- config$VAR2 Note: to avoid warnings make sure the config file has a final line without any indents in it! Update library(yaml) config$var3 &lt;- &quot;VAR3&quot; write_yaml(config, &quot;../config.yaml&quot;) CSV |VAR1 | VAR2| |-----|-----| | 1 | 2 | Read in config &lt;- read.csv(&quot;path_to_config&quot;) #OR library(readr) config &lt;- read_csv(&quot;path_to_config&quot;) var1 &lt;- config$VAR1 var2 &lt;- config$VAR2 Update config$var3 &lt;- &quot;VAR3&quot; write.csv(config, &quot;path_to_config&quot;, row.names = FALSE) Quality Efficiency Note: download R before downloading Rstudio↩ "],
["appendix.html", "Appendix Technical Terminology Resources", " Appendix Technical Terminology Term Definition B Binary package A distribution methods to a developer who doesn’t have package development tools. Also, a single file. Bundled package A package that’s been compressed into a single file C Compiled A program called a compiler, takes the plaintext source file and produces a native binary executable that the computer knows how to run intrinsically. When this happens it is called compile-time, and is distinct from run-time, which is when and executable is run and the program actually does things. D Dynamically typed Variable types are not explicitly declared. Among dynamically typed languages are Python, R, and JavaScript. G Global type inference Types are inferred from context. Explicit type annotations are instead for the benefit of the programmers, and serve as machine-verified documentation. I In memory package A package which has been loaded into memory and then attaches it to the search path Installed package Binary package that’s been decompressed into a package library Interpreted Interpreted languages have programs called interpreters that read plaintext source files and execute the instructions at the same moment. No native binary executable is created. Languages like Python and R are interpreted. L Lazily evaluated Values are not computed until required. This allows for functions to compose together in a performant way (no more work than necessary will be done), and allows for the use of infinite data structures (you could define the list of all prime numbers and only those you actually use will be computed). However, it also makes it more difficult to reason about performance, because it is sometimes not obvious when or if a particular computation will be performed. P Purely functional Every function is a function in the mathematical sense (IE pure). No squinting required. Rather than functions having side-effects, are first-class values governed by the type system. To put it another way, code has only expressions that are evaluated; it does not have statements that are executed. S Source package A directory with all the components Statically typed Every expression has a type which is determined at compile-time. If the types of any expressions don’t match up appropriately, the compiler will reject your program – there are guaranteed to be no type errors at run-time. Statically typed languages include C#, C++, TypeScript, and Java. Resources R General Advanced R - Hadley Wickam Package Development R Packages - Hadley Wickham Data Science R for Data Science - Hadley Wickham Visualization ggplot2 - Hadley Wickham R Graphics Cookbook - Winston Chang Python "]
]
