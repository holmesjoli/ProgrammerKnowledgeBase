[
["index.html", "Preface", " Preface I started thinking about how to be a better programmer while I worked for Mathematica Policy Research. I had the raw technical skills, but I wasn’t actually a very good programmer. My early code broke frequently, contained bugs, and wasn’t easily adaptable when we had to implement changes, which was often and always. As Data Scientists, we’re constantly challenged because we live at the intersection of some many worlds: math, statistics, computer science, subject-matter expertise, data visualization and design. Because we intersect so many different fields and there’s so much to know within each of these specific fields, I’ve found that the raw technical skills of many junior data scientists are often underdeveloped. The goal of this book is to help other junior programmers develop those skills so they too can write, quality, efficient, robust code. But before we dive in, first a big thanks to Eva Ward, Brian Hunscher, Linus Marco, Charlie Hanley, Michael Sisti, and many others! These folks, and many others invested their time to teach me and it was with them that I started to develop the ideas, methods, and content for this book. "],
["about.html", "About", " About Joli Holmes is a Data Scientist at the Texas Policy Lab, Rice University. She has a BA in Economics with a minor in data analysis from Wesleyan University in Connecticut. She loves data. And even more than her love of data is her love for teaching all things data and making complex (or not complex!) concepts more accessible. She currently lives and works in Houston, TX, but is originally from Seattle, WA area. Her favorite programming language is R, but Python is a very close second! Her stretch interests include: data ethics1 and data art! She wrote this book in bookdown. R programmers never cease to amaze! Happy Coding Y’all! My two favorite data ethics books are Cathy O’Neill’s Weapons of Math Destruction and Safiya Umoja Nobel’s Algorithms of Oppression↩ "],
["introduction.html", "1 Introduction 1.1 Some scenarios 1.2 Some solutions", " 1 Introduction Welcome to the Programmer Knowledge Base. Over the last few years, I’ve noticed that young data scientists are often very statistically advanced, but are sometimes programmatically beginners. The goal of this book is to introduce some best programming practices to produce high-quality code. The best practices are first introduced as concept and then are exemplied for several different languages. 1.1 Some scenarios Scenario 1 Imagine that you worked on a project a year, or even several months ago. While you were working on that project you knew everything about it, the ins and outs and all the fine print. Now several weeks, months, or maybe even a year has passed and someone comes to you to ask how you generated the result. You go back to the code to look it up and give them some more details, but then you remember that you left the project folder a little messier than intended and never had to the time to go back and clean it up. In fact, it was a mad scramble until the end, so things are definitely messy. As you’re looking over the output, you try to recall if you turned in final_final_final_v2 or final_final_v3 report. And which code created that output, create_df1_final or create_df2.2? But wasn’t there a bug in create_df2.2 that you found right before the output was due, so you actually used create_df1_final_JH? But maybe another programmer worked on that part with you, so did you use their version or yours? Wow, you wish you had updated the documentation to tell you how to run the program in the first place. Because you vagually recall there was some other script you had to run befor eyou could run create_df1_final_JH. And once you think you’ve maybe figured out which code was used to create which output, you realize all the packages you used have been updated since, so some of the functions you wrote have stopped working correctly. Scenario 2 Another programmer left the company a week ago and you’re responsible for picking up the pieces. You open the folder. There’s a word document with a few vague guidelines about how the program is supposed to run that was updated 3 months ago, when the code as updated 1 month ago. Scripts are poorly named, and you’re really unsure where to start. You open the first script that says main and it’s 4000 lines long. There are some functions in that script, but none of the functions have any documentation. Well this is going to be fun you think as you see for-loops nested in for-loops nested in for-loops … Scenario 3 You’re trying to re-create summarized tables from a code base. The numbers just seem a little off and you want to verify the results make sense. But the code wasn’t version controlled, and the data that were used to create the aggregate tables were overwritten several times because the name was hardcoded and there wasn’t a date or anything else that changed. Any attempt of tracing back the original code to create the original output is gone. Scenario 4 Or maybe this scenario. You shipped the output to the client/partner. Hallelujah! You reviewed the output and your co-workers did too. Several weeks later your recoding something and you notice some of the numbers seem really off, like magnitudes different than the output your sent before. Oh no, you’ve discovered the bug in the code after it gets shipped to the client/partner. Or maybe you discover just before you ship the output to the client/partner, but it’s a mad scramble and everyone has to re-run their individual piece of the pipeline. Yikes!! STOPPPPPP! I think a lot of folks working on teams with data have come across at least one, if not multiple of these scenarios. Even teams that really stress QA/QC can find themselves in these situations. They’re uncomfortable, stressful, and often result in some later work nights than intended. I have personally come across all of these and contributed to several when I didn’t have guidance on how to do things better. Thankfully a lot of smart people, who have spent a longer time than myself thinking about and dealing with these issues, have come up with some great solutions! 1.2 Some solutions The simple solution is this: lets build robust high-quality code. What do we mean by ‘robust high-quality code’? In my mind, there are several parts to robust high-quality code. High-quality code a) is reproducible across three dimensions: people, time, and machines and b) efficient c) tested. Reproducibility “There’s always at least two people on a project, you and future you”. People: code should be reproducible across multiple programmers. Programmer B should be able to run Programmer A’s output and return the same output. Time: code should be reproducible across time. Code run today should still be able to be run a year from now and return the same output. Software/Machines: Code should not be machine specific! This is an especially important concept for those using open-source software. Open-source software can change extremely quickly, and functions or packages that were used a month ago, may no longer work in the same way or have the same functions today. There’s no one way to maintain reproducible code bases across these three dimensions, but there are several best practices that the broader data science, statistics, and computer science communities seem be in consensus on. Specificially we’ll talk about2: Configuration files Documentation Code version control and the software development lifecycle Virtual environments and package managers Building packages Data Archiving Repetition vs. Replication vs. Reproduction Repetition Replication Reproduction Same Lab Different Lab Different Lab Same Methods Same Methods Different Methods Quality Testing is an integral part to being a good programmer, whether you’re building software for an application or building software for data analysis, it is integral to test code. Specifically we’ll talk about: Code Review Assertions Unit testing Integration testing Efficiency Specifically we’ll talk about: Benchmarking Profiling Parallel processing Computing resources (Azure, AWS, etc.) note: these topics are not listed in order of importance, they are all very important!↩ "],
["reproducibility-1.html", "2 Reproducibility 2.1 Configuration files 2.2 Documentation 2.3 Git version control 2.4 Environments 2.5 Packages 2.6 Data access and storage", " 2 Reproducibility 2.1 Configuration files One thing that programmers constantly face is a changing set of parameters that are used as inputs to any script. Whether this is new data, or a new title for a graph, it always seems to change. If these parameters are hardcoded into a script, the likelihood that you’ll be able to remember where all those inputs are is low. Also, it can make debugging more challenging! An easy way to avoid these issues is to use a configuration file. Introduction A configuration file is a file which contains all of our parameters that are likely to change of the lifecycle of the project. Things like paths, variable names, visualization titles, and captions are best stored in a configuration file and then loaded at the top of the script where they are used. Configuration files are also important to version control. Say that you changed the title of a visualization code, but not the actual function that creates the visualization. If the title is hardcoded into the script, then when you submit a pull request the reviewer of the pull request will have to double-check that it’s not the function that changed, but just the text for the title. If that title were stored in a configuration file, and let’s say you made many changes to many titles, then the other programmer only has to review the one file instead of a bunch, and the underlying code base stays unchanged. Using a config file, also makes it less likely that bugs will occur. If you path is embedded down in line 403, and you forget to update it to the most recent datafile, uh oh, wrong data gets used or produced! Formats Different programmers have different configuration file preferences. The common ones are YAML, JSON, and CSV. My personal preference is a YAML file and that’s because you can add comments, which you can’t do with JSON, and you can create a nested format, which you can’t do with a CSV. The nested format makes it easy to loop through different models or dataframes which are going to utilize the same functions. YAML ## YAML files can be commented! VAR1: 1 VAR2: 2 If using nested yaml structure, make sure to indent with spaces, not tabs! Add a final line to a yaml config file, or some software will have difficulty reading it in JSON { &quot;VAR1&quot;: 1, &quot;VAR2&quot;: 2 } JSON can’t do comments Web-developers will often use a JSON file because JSON stands for JavaScript Object Notation CSV VAR1 VAR2 1 2 CSV files can be really handy ways to organize script parameters, especially if you’re working on a team with people who aren’t used to working in a JSON or YAML file One limitation of a CSV is their flat structure, it’s often really nice to be able to nest attributes within other sections of a config file 2.2 Documentation Documentation is an important step to making code reproducible. Document your code and systems for yourself, other programmers/team members, and maybe most importantly: your future self. It’s unlikely you’ll remember the exact steps you took to produce some output a year from now, but that’s what documentation is for! Repositories Repositories are documented with a README.md file and stored in repository root directory. my_repo\\ README.md The .md suffix indicates a markdown file. If you have used markdown before here are some resources: Example README Markdown syntax Markdown table formatting Udacity Course on READMEs Markdown is an important skill to learn. Many technical documentation systems are created using markdown because it’s easy to learn and can be version controlled with ease unlike other word processing software (e.g. Microsoft Word, or Google Docs). README READMEs are documentation for any humans who will need to use that code, including: Yourself Your future self Co-workers or other people who will contribute to your code People who will use your code Clients if the code will be delivered to them at the end of the project (some code technically belongs to clients) On any version control system (e.g. Github, Bitbucket, TFS) the README will be rendered as HTML and displayed on the repository page. General README Structure In general, READMEs should follow this format, but they of course can deviate to provide more or less information where necessary. A paragraph describing the high-level purpose of the package An example showing how to use the package or repository of code to solve a simple solution Installation instructions An overview that describes the main components of the package General questions to answer in a README: What are the exact steps that need to be taken to get up and running? What should someone already have installed or configured? What might someone have a hard time understanding right away? Any known issues or bugs? Where are the data for this analysis stored? What do we expect those data to look like? Code Functions Always document your functions. Include a short description about what the function does, what parameters the function takes, what the function returns, and even a short example! If you find yourself have trouble summarizing what the function does in a short description, this is a good indicator that you need to break that function into multiple parts. This is also very important for testing, which we’ll get to later on! Code-specific documentation best practices will be discussed in the 03-languages section. Commenting Code One of the first things many programmers learn is to comment their code, to provide information about the steps they are taking. While we want to encourage commenting code, we also want to be careful. Some programmers will write huge blocks of text before executing a number of steps. If this is you, we want to make things more modular. One line should be able to describe any function. Data Data Dictionaries Like code, it’s important to document your data with a data dictionary. A data dictionary contains information about variable names and their corresponding meta data. A data dictionary should always contain labels, as variables often aren’t easy to identify what data they represent. A data dictionary can also contain information such as: variable type (e.g. numeric, string, date, or integer, float if you want to get more specific), variable length, allowable values for descriptive variables, units of measure for numeric values. Data Dictionary Resources: Best Practices Systems One of the more challenging things to document is a system. Generally speaking, if we’re talking about a system, then we’re talking about a team of people responsible for developing and maintaing a code base, as well as other documentation. If possible, it’s best to store code repositories at the project level. So all of the repositories that contribute to a system are stored under the same project. Then at the system-level create a README to document how the system works. General questions to answer: * How the output from one repository used as an input in another repository? * What tests are run in-between? * What are the necessary parameters for each section of a system to run? * What is the timeline of the system run? Hours, Days, Months? 2.3 Git version control Introduction Version control systems allow for easy access to prior versions of code, branching for development, and easy of collaboration between programmers. Different version control systems exist across several industries, but it is considered best practice to use a git as your version control system for any coding task. Git is important to use to keep track of code version histories. Code isn’t static and just written once and executed, but is consistently changing. Version control is necessary to keep track of these updates. Repository set-up Each repository should start with these branches master All code in this repository has been tested and is solid This code has been ‘released’ develop The develop branch is created from the master branch This code is has also been tested and is solid but is pre-release feature branches Feature branches are stored in the features/ folder Feature branches are created from the develop branch This code is in development It may be pre-tested right now and is still in progress WorkFlow Download the code base Download the code base from the remote repository. This is called pulling. Navigate to the folder where the code base should be stored by using the cd command. git clone path_to_repo Checkout a branch Checking out a branch is like opening a file. A branch is a group of files that are the “Saved As” version of the files on the master or develop branches. git checkout branch_name Make changes to the code base Edit your code. Remember to test the code before commiting it. Try to avoid commiting broken code! Review which files changed We always want to review which files changed, to make sure we don’t miss any changes that other code relies upon. git status Review changed files Review the changes. Imagine you had track changes on in Word and you can see the old and new versions. git diff file_name Stage the file(s) for commitment git add file_name Write a commit message Make the commit message short, and descriptive of the why, not the what! Remember that commits should be frequent enough that “and” isn’t needed in the commit message git commit -m &quot;commit message&quot;. Update remote repository Repeat all prior steps until you’re ready to update the code on the remote repository. git push origin branch_name Pull Request Get another programmer(s) to review the code by submitting a code request to the develop or master branch. TODO: Add more information about PULL REQUESTS Checkout changed code Once the pull request has been accepted pull down the updated code from remote. git checkout other_branch git pull other_branch Visualizing Git Flow Image Source In-depth development workflow A develop branch is created from master Feature branches are created from develop When a feature is complete a pull request is made for a potential merge into the develop branch This request goes through a code review and can be approved/rejected by the technical lead When develop branch is complete for a given cycle, a release branch is created from develop When the release branch is tested and deemed ready for production it is merged into develop and master If an issue in master is detected a hotfix branch is created from master Once the hotfix is complete a pull request is made for a potential merge to both the develop and master. The Dos and the Don’ts Git Don’ts Version control sensitive data This is a security concern. Servers which host remote repositories are not necessarily as secure as the network the data are stored on. Additionally, storing the data in more than one location increases the likelyhood that a data breach could occur. Use git as a data versioning tool Git is not a data versioning tool. This book will discuss data versioning at a later point! Storing data on the git (even unsensitive data) will clutter the repository’s history. An exception to this rule may be storing static metadata (e.g. config file) or fake data used for unit testing, along with some other well thought-out special cases. Think very carefully about whether there’s a good reason to put data in the repository and seek guidance before doing so. Version control binary files Any file that cannot be opened by a text editor (Notepad, Atom, Sublime, etc.). If you are unsure about whether a file is a binary file, just try to open it in Notepad—if you see a bunch of gibberish then it is! If you see a proper text representation of what you know the file contains, then it isn’t! Git is a version control tool, meaning that it tracks changes to the contents of a file. This works great for text files, but not at all for binary files. If you change a single character of a Word document, all of the contents of the binary file will change. This means that git will not be able to show you what changed about the file, only that the file changed. Don’t version these file types: Word documents (.doc, .docx) Excel files (.xls, .xslsx, .xlsm, .xlsb, …) Images (.jpg, .png, …) Executables (.exe, .dll, …) SAS/Stata/other data files (.sas7bdat, .dta, .dat (sometimes), …) There can be exceptions, but generally it is bad practice to version control this type of file. They, like data, will bloat the repository for no good reason. Version control temporary files Windows and other programs often create temporary files when a file is in the process of being written or when a team member has the file open. They often have weird names starting with “~$” or extensions like “.bak”. Don’t version control these as they are temporary! Version control output files This includes log files and other direct program output. Like binary files, these files are likely to change a lot when they change, but they are also conceptually more like data than they are like code. Use name versioning for files tracked by git Code should never be names _v1 or _v2_final or _v3_final_final_JH. Git takes care versioning for you! The days of archiving code and renaming different versions are over! Just make your change and commit it. Then, to access the previous version, just use git checkout or use git diff or git show to view changes. Gits Dos Write concise and informative commit messages Commit messages are the easiest way for other collaborators and future versions of yourself to figure out what changes were made in each commit. This makes it easier to trace errors, keep track of features, and find explanations for changes. Remember the commit message should explain the why not the what. Make each commit a single cohesive change Conceptually, each commit should be a single unit of work (e.g. “fixed bug X” or “added X section to report” or “changed working in section X”). This unit of work could be small (a couple lines of code, or even fixing a typo) or it could be large (adding an entire section to a program), but all changes/additions/deletions within a commit should be related. A quick rule is that you should rarely need to use the word “and” in a commit message—if you are, then you should probably make that commit into two or more commits. Doing this helps everyone get a better sense of what each commit changed, but also makes it easier to inspect and roll back changes. We wouldn’t want to have to undo a change unrelated to the one that actually needs to be reversed When adding files to be committed, use git add [file/s] to add only certain modified files to staging. Use git reset to unstage all files if you accidentally add too many Review all changes before committing This quick and simple step should make sure that you avoid all of the “do nots” above 99% of the time. First add the files you want to commit: git add [files] or git add . to stage all files for commit Review the changes: git status to see a list of all files that will be committed, and git diff --cached to see a detailed view of all changes to each staged file Commit! git commit -m [my concise and informative commit message] Use a .gitignore file A .gitignore file that tells git which file types not to track. Git will ignore any files that match patterns specified in the .gitignore. Gitignore A .gitignore file tells git which files should not be tracked by version control. Common files extensions to inlucde data file extensions, binary files, executables etc. If a .gitignore is included in the working directory the command git add . will only add files where the file extensions do NOT appear in the gitignore. Create a .gitignore In the command line/terminal type echo &gt; .gitignore To create the file manually, save the files and folders that start with a period with a trailing period. E.g .gitignore. Where to put the .gitignore Generally, you want the file location to be at the root directory of the project, in the same directory as the .git folder Extensions often included in a .gitignore Data *.csv *.xlsx *.docx Julia Github Julia suggestions R Github R suggestions Python Github Python suggestions SAS *.sas7bdat Stata *.dta Git still tracking files marked in gitignore This happens because these files were originally add to git. Even if the files are removed from git and added into the .gitignore, git still wants to track the files. Here’s the stack solution. Common Commands Action Git Command Branching (like a Save As for code) git branch new_branch_name To pull down a local version of a remote branch and switch to that branch git checkout --track origin/remote_branch_name To fetch all branches git fetch --all To rollback to an old Git commit on the repo. Revision is the commit hash. git checkout [revision] . To create and checkout a branch git checkout -b branch_name To undo the rollback to an old Git commit on the repo git reset --hard To check which tracked files have been changed git status Compare code changes between previous commit and current code git diff name_of_file To stage a modified file to be committed git add name_of_file To un-add a modified file git reset HEAD To commit staged files to your local repo without opening VIM to write a commit message git commit -m 'commit message' To push your local repo to the remote repository git push origin local_branch_name Amend or change a commit message git commit --amend To push the new branch to the remote git push --set-upstream origin &lt;new_branch_name&gt; Delete a branch on remote (post-merge w/ master and post-code review!) git push --delete &lt;remote_name&gt; &lt;branch_name&gt; Delete a branch locally (post-merge w/ master and post-code review!) git branch -d &lt;branch_name&gt; VIM Editor The VIM (Vi IMproved) editor is a text editor for unix/linux operating systems. However, sometimes when using git commands the editor will appear. Examples of when it appears are during merges, or editing commit messages. One of the only commands you need to know with the VIM editor is how to save and close. To do so: hit the Esc button on the keyboard. And then type :wq to save and exit. Additional Resources: save and close cheatsheet Practice Practice using git commands to version control code with the repository here. Glossary Term Definition B branch An active line of development. More literally it is a local copy of the code. Branching off of the master branch preserves the stability of the code. bitbucket An Atlassian another server used to host code C checkout The action of updating a branch clone The action of copying the repository hosted on server to a local computer commit A single point in the code’s history. It contains a hash #, which is unique and a commit message which indicates why changes were made to the code base. F fetch To get the branch’s head from the remote repository G git Version control system a tool to manage your source code history github A public server to host vresion controlled code gitignore A file used to indicate which file types should not be tracked by git L local The version of the code that exists on your computer; your local repository M master The default development branch! You can think of master as the stable version of your code. If you need to make updates/changes to the code, consider making those changes on a branch and then merging that branch into master once your code has been code reviewed and tested for bugs. merge To bring the contents of a different branch into the current branch. O origin The upstream (original) repository. P pull To fetch a branch and merge it. push To send the changes from the local branch to the remote branch. R remote The version of the code that exists on the server; i.e. the remote repository. repository (repo) A collection of code, commits, branches, and tags. S stage Code must be staged before it can be committed. T tag A reference typically used to mark a particular point in the commit chair. U unstage V VIM Resources Udacity Git Class Git Flow Git Book Linus’s Training3 Atlassian Gitflow 2.4 Environments Note this section is only application to users of open-source software. SAS and Stata users, you can skip this section, unless of course you’re interested in learning more! Introduction The cool thing about open-source software is that anyone with a knowledge of that software can help build packages for the software community. Open source software is amazing, because there’s a huge community of programmers out there trying to achieve similar goals given similar constraints. There are however, some big downsides to utilizing open source software. One of the biggest is that developers of packages churn out updates that may or may not be backwards compatible with older versions of their packages. Packages are rolled-out continuously and aren’t built as part of one central, commercialized system, which can result in version control issues for software users. And it goes beyond packages! Python rolled out version 3 of the language in 2008.4 Yet, a lot of Stack Overflow answers are still written using Python 2. When I try to run Python 2, I get errors, which then have to be debugged.5 Fortunately, there’s a simple solution: multiple development environments! To make code reproducible across time and across machines, it’s best to work with multiple environments. An environment takes a snapshot of the user’s system at that point in time. Others can then generate the same environment and still be able to run code, even as packages and languages continue to change and develop over time. Using Multiple Environments Code that doesn’t break Ease of collaboration Client deliverables Read this article to learn more! 2.5 Packages Introduction Have you noticed yourself recycling code? Maybe you go back to an older project and copy a bunch of the functions over to the new script you’re working in. If this is you, it’s time to write a package! Developing packages is an important skill because it saves time, but also reduces the likelihood of introducing bugs into the code. All code should be tested, 3.1, but it should really only need to be tested once. Packages are also a great way to update code utilized across multiple projects. Say you optimized a function, and now it runs in half the time it took before, well we don’t want to copy and paste that code to all the projects that used that original function. Instead, we just update the package, and it also long as there we’re any major developments to the package it should be smooth sailing. Many new programmers fear writing packages because they assume it’s a very advanced technique. Although, it is more advanced, it’s really not very different than writing scripts. It’s just a very nicely packaged bundle of all those scripts. Versioning Packages have versions, with smaller numbers signaling earlier versions of the package. The in-development package has 4 numbers: major.minor.path.dev Released packages don’t have a dev component Patch Versions A patch is used when bugs are fixed without the additional of any new significant features. However, most releases will have a patch number of zero. This could look like (0.8.0 to 0.8.2) Minor Release A minor release can include bug fixes, new features, and changes in backward compatibility. Minor releases can have two or even three digits. This could be (1.17.0 to 1.20.0) Major Release A major release includes changes that are not backward compatible and are likely to affect many users. 2.6 Data access and storage Data access workflow The current data access workflow for many researchers and analysts looks like this: Go to a user-interface and request specific data Downloading that data as a csv, or other file format Copy that data to a project-specific data folder Launch statistical software and hardcode file path to the data folder Read in data Get really frustrated when a code worker changes the file structure which then throws everything else off! Okay, so step 6 might now always be the case, but it often is!! If you’ve spent a lot of time on collaborative projects, that’s almost undoubtly happened to you. So, this isn’t actually a great workflow. There are many point where things can go wrong and make the analysis not reproducible. Reasons why this isn’t a great workflow: Step 1: I personally hate user interfaces for requesting data. It can be very challenging to reproduce analyses because it’s hard to know exactly what settings the previous user set-up to request the data. Without knowing these precise settings, it’s likely that the new data extract will be a little different than the previous extract. Step 2: I always to choose to download as a .csv because it can easily be read into any statistical software: R, SAS, Stata, Python, you name it. However, large amounts of data being stored in csvs are not at all efficient. It can take time to download the data and then transfer the data from the Downloads folder to the project folder. Step 3: Copying the data from downloads to a project-specific folder is normally manual. We want the whole data pipeline, from the initial extract to the final reporting to be as automatted as possible. Step 4: Hardcoding!! Everyone hardcodes file paths, we have to! But it’s dangerous … how many of us have had that path change when we’re in the middle of an analysis, or after we finished an analysis 3 months ago and everything has changed. If you do have to hard code file paths, put them in a config file!! Step 5: Read in data. Is it a .csv, .xlsx, .Rdata, .dta, .dat, .sas7bdat, .tsv, etc.? There are so many different file formats and ways to read in data, it can be irratating, especially if your data is in multiple file formats. Data storage workflow Peform analysis Write out data as a .csv, .xlsx, .Rdata, .dta, .dat, .sas7bdat, .tsv and probably use some sort of file naming such as data_20190507.csv. Have to update paths every single time the data are used downstream. Get frustrated when the person upstream changes the naming structure. So this also isn’t a great workflow. How many folks have had a data folder filed with a bunch of files with very similar sounding names, and they have extensions such as _old, _v1, _v2, _final, etc? Then it’s confusing if the final deliverable came from the data versioned data_20180917 or from data_20180912? Although it’s often the case that the most recently generated data is used to create the final output, it’s not always the case. Reasons why this isn’t a great workflow: Upstream processes that write out data are subject to change input names Downstream processes that read in data are subject to input name changes Often results in very messy Data/ folders Isn’t always clear which data were used to create the final deliverable Different file formats mean programmers have to adapt to upstream/downstream programmers needs Accessing data The easiest way to access data stored in another organization’s server is probably through the utilization of an application programmable interface (API). APIs What is an API? APIs are a little hard to understand if you’ve never come across them before. Google defines an API as “a set of functions and procedures allowing the creation of applications that access the features or data of an operating system, application, or other service.” This is a very technical, but accurate definition. Simply put, an API allows programmers to access data stored on remote servers through code. Example request At it’s most basic an API looks something like what’s outlined in code before. Generally, the user will need a token which they get from the organization they’re requesting data.6 library(httr) response &lt;- httr::GET(&quot;https://website.server/data/get=?Var1,Var2&amp;key=token&quot;) The first part of the made-up string below https://website.server/data/get=? is called the endpoint and tells the HTTP request which website to get data from. The next part of the string Var1,Var2&amp;key=token are different parameters. In this fake API, I’m requesting Var1 and Var2, and providing my token as authentication. Each API is set-up differently, so it’s very important to read all documentation before you start. Most APIs have sample code, often in several languages, to get started! Note: the data will be returned as a list, so that additional metadata can be returned with the request. So there’s a little more data processing once the data are returned from an API. Why are APIs so useful? At a first glance, an API may seem like an extra amount of work for a programmer. Utilizing an API to access data combines steps 1 - 5 outlined in the data access workflow section, and completely omits step 6! The other reason we love APIs is that that code will be version controlled, so any other programmer will be able to see how you set-up the request and exactly which data were requested.7 API Specifics HTTP Verbs These are commonly used verbs when making requests to an API. When requesting data from a server, most users will be utilizing the GET verb. Verb Interpretation GET read POST create PUT replace/update DELETE delete Status codes APIs always return a status code as part of the request. When a request goes through it returns a status code of 200. The status response codes can be helpful when debugging if a request failed, which can help freuqently if the parameters are set-up exactly as the API is expecting them. Code Interpretation 200 OK - Response to a successful REST API action. The HTTP method can be GET, POST, PUT, PATCH or DELETE. 204 No content 400 Bad Request - The request is malformed, such as message body format error. 401 Unauthorized - Wrong or no authencation ID/password provided. 403 Forbidden - It’s used when the authentication succeeded but authenticated user doesn’t have permission to the request resource. 404 Not Found - When a non-existent resource is requested. 405 Method Not Allowed - The error checking for unexpected HTTP method. For example, the Rest API is expecting HTTP GET, but HTTP PUT is used. 429 Too Many Requests - The error is used when there may be DOS attack detected or the request is rejected due to rate limiting. Source Server Access Accessing SQL servers etc. SECTION UNDER CONSTRUCTION Outputting Data Server The best way to store data is in a relational database system. However, this isn’t always possible for organizations that may not have access to a relational database system. The next best method is some sort of data archiving structure like the one outlined below. Most people just write out the data to the Data/ folder. However, in large projects the data folder can quickly get messy. This creates problems like the ones outlined in the data storage workflow section. Suggested folder structure Data/ Processed/ Current/ Archive/ Raw/ In the data folder there are two folders: Processed and Raw. Raw contains the raw data and processed contains the data that have been validated and managed. Within the processed file there are two folders: Current and Archive. The Current folder contains the most recently processed files. The files in this folder DO NOT have time specific names. There aren’t any data like summary_data_20180912.csv. Instead there’s just a file that says summary data.csv. However, in the Archive folder, there are timestamped zipfiles containing all the data that were generated on a single day. All the zipped files in this folder are named systematically such that it’s easy to tell when the data were create and maybe why! Example: Data/ Processed/ Current/ data.csv summary_data.csv summary_stats.csv log.txt Archive/ 20180912.zip 20180917.zip 20180922.zip Raw/ If we were to open up 20180912.zip it would look like this: data.csv summary_data.csv summary_stats.csv log.txt Wowza, this structure solves a lot of problems! We don’t have to worry about filenames downstream having to get updated because they’re pointed at the Current folder. At the same time, we have a record of all the data we created, that way if we delivered output generated from data 20180717, and the client/partner has a question about the data, we can easily open it up and access it. Automated Commits If we follow the structure outlined above, out data archiving system is going to be great. But it can actually still be improved. There is still linking the code to the data to the output. Say several months have passed and our client/partner comes back to us with a question about how a specific variable in summary_data.csv was coded in version 20180917.zip. However, the codebase has changed since we delivered the output several months ago. We can probably trace the code back, since of course all code is version controlled, and then we can checkout the old code and compare it to the new code. However, that seems like a lot of work, and we made multiple commits the day the code was created, so it’s hard to know exactly what code was used to create the output. What’s the solution? The solution is to include a commit hash in all of zip_files names in the Archive folder. So when we update the code, we make a commit to the remote repository. This commit hash is a unique identifier that points to a version of the code at a specific point in time. That hash then gets used to link the code to the data and the data to the output. Now our Data and Output folders look like this: Data/ Processed/ Current/ data.csv summary_data.csv summary_stats.csv log.txt Archive/ master_t5o985_042018.zip master_5986j6_052018.zip master_9jfd86_062018.zip Raw/ Output/ Processed/ Current/ fig1.png fig2.png fig3.png Archive/ master_t5o985_042018.zip master_5986j6_052018.zip master_9jfd86_062018.zip Raw/ If we send the client/partner master_t5o985_042018.zip and there’s a question about how data or a figure got created, it’s super easy for us to back track to both the data and the code. It’s recommended to set-up the archive folders as the in following format: branchName + _ + commit hash + _ + date/description. The date/description part makes it easy for you to find a specific version of the data/output without knowing the exact date or why the output was created.8 Both R and Python have the ability to make automatted commits. Resources APIs What is an API in English please? Linus was a previous co-worker of mine when I worked at Mathematica Policy Research↩ https://learntocodewith.me/programming/python/python-2-vs-python-3/#history-of-python2-vs-3↩ 99% of the time this is related to the print statement and takes 2 seconds to change, but the example of why this is an issue remains!↩ There are several ways of authenticating users, the token method is the simplest, but also less secure than other methods. If you’re access confidential information that isn’t publically available, you’ll most likely have to go through another type of authentical called Oauth.↩ It’s important not to version control your API tokens/keys/secrets. Then anyone else can pretend to be you and access the data. Many APIs have request per hour/day limits, so don’t share keys! Add keys in a separate configuration file that isn’t version controlled, and then add a sample key file to your repository to show users how you set-up that configuration file.↩ I worked on a project one time where we produced data from the client each month, and we named all our files as with the description as “master_commit hash_May Monthly”, “master_commit hash_June Monthly”, etc. Then if I need data from Jan - June, it was very easy for me to go in and figure out which data were created when.↩ "],
["quality-1.html", "3 Quality 3.1 Code Testing 3.2 Code Style 3.3 Code Review 3.4 Resources", " 3 Quality 3.1 Code Testing Introduction Very recently, I was trying to compare two lists with numpys nan object in both lists. I forgot that in python np.nan == np.nan returns False.9 However, I didn’t remember this when I was writing my code. I remembered that when I was testing my code. It’s fairly easy to assume you know the behavior of a function, as I did above and accidentally introduce bugs into the code. Testing is an important way to automate the discovery of such mistakes, and shift the timeline of bug discover from later to earlier. Most programmers I know do test there code, but they often do it in an adhoc way. To ensure that we’re creating output from derived from quality code, we want to test our code in consistent and standardized way.10 Testing Pros Fewer bugs Testing makes it explicit how the code should behave Better code structure Code that is easy to test is designed better Testing forces the programmer to break up complicated parts of the code into separate parts that work in isolation Easier restarts and collaboration Easier to pick-up where you left off if you know what’s passing/failing Other programmers can figure out the goals of the code based on the tests and feel more confident making changes and re-running the code because the tests are passing Robust code Adds confidence that you know what your code is doing Best practices Tests should be independent, chill, implementaiton agnostic, fast, and shareable. Term Definition Independent Tests shouldn’t depend on other tests, or auto-fail when other tests fail. This makes is hard to tell what code needs to be worked on. Chill Tests shouldn’t be super strict on timing or require super precise output. Very relaxed tests are often still quite good. Implementation agnostic Tests shouldn’t assume implementation details and test those details. Fast These tests are meant to be run constantly, many times per day. Longer end to end testing should be done as a separate step. Shareable Don’t test with any sensitive data. You want to be able to communicate the results of your test without risk. Testing timeline Testing is often one of those things that is thought of last not first, often after a bug has been discovered! A better strategy is to build testing time into the development timeline. And test as you program, don’t leave it all until the end. Testing as you go along also helps to re-organize code in a more efficient way. You would never submit a paper or report after the first draft. Code shouldn’t be submitted after the first draft either! It needs to be edited and thoughtfully reorganized throughout the development process and testing can help encourage this practice. Types of tests Assertions An assertion is statement that evaluates to true or false. If the statement evaluates to true then the assertion passes. If the statement evaluates to false then it throws an assertion error. Assertions are the building blocks for unit tests and integration tests. At minimum, code should have strategically placed assertions throughout. However, it is much preferred to have an organized set of test files Unit Tests A unit test is a test written by the programmer to verify that a relatively small piece of code is doing what it is intended to do. They are narrow in scope, they should be easy to write and execute, and their effectiveness depends on what the programmer considers to be useful. The tests are intended for the use of the programmer, they are not directly useful to anybody else, though, if they do their job, testers and users downstream should benefit from seeing fewer bugs. Part of being a unit test is the implication that things outside the code under test are mocked or stubbed out. Unit tests shouldn’t have dependencies on outside systems. They test internal consistency as opposed to proving that they play nicely with some outside system. Definition from Stack Overflow Integration Tests An integration test is done to demonstrate that different pieces of the system work together. Integration tests cover whole applications, and they require much more effort to put together. They usually require resources like database instances and hardware to be allocated for them. The integration tests do a more convincing job of demonstrating the system works (especially to non-programmers) than a set of unit tests can, at least to the extent the integration test environment resembles production. Actually “integration test” gets used for a wide variety of things, from full-on system tests against an environment made to resemble production to any test that uses a resource (like a database or queue) that isn’t mocked out. Definition from Stack Overflow 3.1.1 Continuous Integration (CI) Continuous integration is process of automattically checking The main idea behind continuous integration is that the longer a branch is checked out, the more likely there will be integration issues when it’s merged back into the main code base (development, or master branches). If there are many changes made on a branch, and there are many developers working on different branches, then Continuous integration is especially useful for teams of developers, but is also useful for data scientists. Even if you’re the only maintainer of a package there’s a lot of automatic testing that can help catch bugs or inconsistencies in code. For example, I recently build a package in R. I used the devtools::check() function before I built the package and everything was passing. However, when I tried to build the package using CI software, the build failed. I forgot to update some of my function documentation when I changed the naming structure of several functions in my package. A code base can never have too much automatted testing! 3.1.1.1 Sources of CI Travis-CI CircleCI AppVeyor Coverage In software development the term “coverage” is a measure indicating the amount of testing performed by test set. Measuring coverage is helpful in several ways. First, the amount of coverage a package has is a useful indicator to other programmers about the quality of the package. If a package has high coverage, then it’s likely a higher quality package as the majority of the code contained in the package is being tested appropriately. Measuring coverage is also very useful for developers. Coverage can help identify modules and functions in a package which aren’t thoroughly tested. 3.2 Code Style Part of having high-quality code is maintaining a consistent programming style across people, and repositories. Having consistent style makes code more readable, and therefore easier to catch bugs and errors. Many languages will have style guides which are maintained by organizations responsible for maintaining the code. You should take the time to read through the style guides of the languages you program in most frequently. Linters Once you read through select style guides, you’ll find out that there’s a lot to remember and keep track of! Luckily, programmers are very lazy people, and have developed packages, often known as linting packages, to help automate the process of inconsistent programming style. If you’re using a IDE such as VS, these linters often come in the form of add-on extensions. 3.3 Code Review A code review is the chance to walk-through and explain your code line-by-line to others involved with the project. Code reviews provide opportunities for other programmers or researchers to help catch bugs and improve overall code quality. Review participants The original author(s) of the code being updated At least one more senior or programmer of an equivalent level People knowledgeable about the content area of your code At least one person who is very familiar the programming language General tips for success Before: If there are data involved and you wish to run the code line-by-line, pre-load the data (or at least a sample of the data) Comment the code as this will make it easier to walk through or explain things on the spot Make sure all the functions have documentation Show that all the tests are passing Modularize! Make functions, have multiple scripts, write a package. This will make it easier to walk through. During: Take notes in the code when your reviewers give feedback, or ask one of the other people involved in the review to take notes for you. After: Ask for clarification, especially if you’re new to the project or software you’re using. More experienced programmers know what they’re doing, but they may be unaware of your confidence in a language/project Once you’ve incorporated the feedback, make sure to ask someone to review the changes Automatic Code Review There are a number of good automatic code review tools available to developers. Most of them are free to use for open-source software, but require a membership for private repositories. Connect your source code to these automatted code review softwares and let them analyze potential errors, inefficiencies, and style. Using an automatted code review software can also help you discover weaknesses in your coding style and when code can be improved. Codacy Codebeat SonarQube One large limitation of most (all?) automatted code review software is that it’s normally only available for more traditional development softwares. But if you’re using a development software, be sure to utilize an automatted code review software. For statistical softwares we’re out of luck. Note, use of such software should not replace the traditional code-review process, especially for data science pipelines which often rely on very specific logic when creating new variables and running models, but it is a quick way to start developing high-quality code. 3.4 Resources Testing Automatic Code Review Continuous code quality and automated code review tools Continuous Integration Continuous Integration Coverage Test Coverage in Software Testing Nan stands for not a number.↩ There’s a term on github called coverage, which refers to the percent of functions which are tested in the repository. A high coverage percent is often an indicator of stable, high quality code.↩ "],
["high-performance-computing.html", "4 High Performance Computing 4.1 Profiling 4.2 Parallel Processing 4.3 Computing Resources", " 4 High Performance Computing 4.1 Profiling 4.2 Parallel Processing Most modern day computers are equiped with multiple processors (CPUs), which are in-turn equipped with multiple cores. Parallel computing makes use of these multiple processors by splitting computationally, memory, or time intensive jobs into independent parts which can run in parallel across the multiple cores of a computer.11 The basic process is the split the job into discrete independent parts, parallelize these jobs across the computer’s multiple process, then combine the results back together. This technique is known as ‘split-apply-combine’ and illustrated in the image below. Image Source Adding additional processors doesn’t reduce the job run-time linearly. If the job takes 1 hour to run on 1 processor, it won’t necessarily take 30 min to run on 2 processors. This is because it takes time to copy the code to different processers, in addition to splitting the process in the beginning and combining the results back together in the end. Setting up a parallel processing job also takes a little bit more time, so it’s up to the analyst to determine if it’s worth the extra effort to set-up a parallel processing job to save time or if running the program serially will suffice. 4.2.1 Hardware Terminology Node Processor/Socket/CPU Core A single motherboard, with possibly multiple sockets the silicon containing likely multiple cores the unit of computation 4.2.2 When to parallelize The job is too slow The job is too large The job has many computations, one task is reasonable, but there are many tasks Each task/job can easily be divided into discrete parts that are completely independent of the other parts 4.2.2.1 Concrete examples Reading in very large data that is in multiple datasets Writing out very large data that is in multiple datasets Bootstrapping or any sampling that requires many multiple random runs Fitting models with very large data. If the data are large enough, the data maybe able to be split into random partitions, modelled, and then knit back together. Fitting multiple models with different outcomes Cross-validation 4.3 Computing Resources There comes a point where even if the code is optimized and parallelized there may still be memory and computation issues. The data are just too big, or the model is too complex, or there are just too many jobs to run in a short turn-around time. Many users are turning to cloud-based infrastructure such as AWS (Amazon) and Asure (Microsoft) when they run into these issues. Cloud-base infrastructures are becoming very popular because users can “rent” computers with very large compute and memory resources for relatively inexpensive. This is a path many larger organizations are choosing to take instead of owning the infrastructure outright. AWS Amazon Web Services (AWS) instances are virtual computers. The benefit of using a virtual computer, like an AWS, is that the instance isn’t physically owned by the user, but instead “rented”. Users rent computer resources (CPU, Memory, etc.), rather than buying an expensive resource that wouldn’t be utilized 100% of the time. Instance Types There are many instances available for use. The CPU on the instances ranges from 2 to 96. The memory on each instance ranges from .5 GB to 488 GB. Similarly the amount of storage on an instance differs vastly across instance. More detail on different instance types can be found here. Increases in computational power (CPU) and memory cost more, so it’s important to choose an instance that will satisfy the memory and power requirements for the analysis, while minimizing cost. Instance Pricing Amazon instances are priced in four ways: On-Demand, Reserved Instances, Spot Instances, and Dedicated Hosts. More detail about different pricing types can be found here. Verify that the pricing for the instance is On-Demand with the person that set-up the instance prior to usage. This link (On-Demand pricing) details how much an instance costs to use per hour. Use Cases Custom configation of a computer in terms of your needs Calling scripts from AWS preserves local computer’s memory and CPU resources Set-Up Before getting started communicate with the person setting-up the instance what your needs. Things to think about before setting-up the instance: Software: E.g. does the instance need R, Python, Julia, Git? If it needs Julia, do you also need Atom? Computational: What are the computation needs? Will you be using parallel processing and utilizing multiple cores? Storage: How big are the data that will be analyzed? How much space with the output take up? Memory: What are your estimated memory requirements? How big are the data to be analyzed? What type of models will you be performing? Workflow These are the basic steps one will need to run an analysis on AWS. These steps are detailed in greater detail in the subsections of this section. Create an instance Start an instance Connect to the instance Migrate data locally to the instance Develop analysis code This can be done locally using a random sample of the data if the data are too large Develop code on a small AWS instance using a random sample of the data Change instance to a larger instance for analysis Run analysis Move output from instance to local computer Disconnect the instance Stop the instance Terminate the instance Start Instance Log in to your AWS account at this link Search for or Click on EC2 Under Resources click on Running Instances Start the instance be selecting the instance of interest (in this example 1B). Then under Actions, under Instance State click Start. Verify the instance startedby checking the instance State and Status Checks Note optimal parallelism doesn’t always mean using all the cores available, as compute power may be necessary for other processes, running simultaneously. Often times, analysts will set the cores in use to be nC - 1 to leave compute resources for other processes.↩ "],
["python-1.html", "5 Python 5.1 Introduction 5.2 Reproducibility 5.3 Quality 5.4 Data Management 5.5 Command Line Interface (CLI) 5.6 Web 5.7 Resources", " 5 Python 5.1 Introduction Python is primary used for web testing, pipelining, data management, data science, and visualization. Installation Download Python 3.7.0 from Anaconda Open Bash and check that Python is saved to the AppData folder where python should return something like C:\\Users\\jh111\\AppData\\Local\\Continuum\\anaconda3\\python.exe where pip should return something like C:\\Users\\jh111\\AppData\\Local\\Continuum\\anaconda3\\Scripts\\pip.exe If either of these throw an error, like “could not find files…”, python and pip need to be added to the PATH environment variable Anaconda Background We use the Anaconda distribution of python. There are several advantages to using Anaconda over base python, including: An Anaconda installation comes with pre-built binaries for many popular packages, including several that we use frequently (numpy, pandas). Installing some of these (e.g. numpy) without Anaconda is very tricky and requires administrator permissions. Anaconda comes with very nice environment management tools, which not only allow you to manage your package dependencies per-project, but also your python version. 5.2 Reproducibility Config files YAML Below is example code of how to read in and write out yaml files. The same functions, documented and tested, can be found in the utilsPy package. Example file ## You can add comments to YAML! VAR1: 1 VAR2: 2 Install conda install pyyaml Import import yaml with open(&quot;config.yaml&quot;, &quot;r&quot;, encoding=&quot;utf-8&quot;) as cf: config = yaml.load(cf) Update import yaml config[&quot;var3&quot;] = &quot;VAR3&quot; with open(&#39;config.yaml&#39;, &#39;w&#39;) as fp: yaml.dump(config, fp) Source JSON Below is example code of how to read in and write out json files. The same functions, documented and tested, can be found in the utilsPy package. Example file { &quot;VAR1&quot; = 1, &quot;VAR2&quot; = 2 } Install conda install json Import import json with open(&#39;config.json&#39;) as cf: config = json.load(cf) CSV Example file |VAR1 | VAR2| |-----|-----| | 1 | 2 | Install conda install pandas Import import pandas as pd config = pd.read_csv(&quot;path_to_csv&quot;) Documentation Project /task-level Each project’s code should live in a repository (see GIT section) Each repository should have a README What are the exact steps that need to be taken to get up and running? What should someone already have installed or configured? What might someone have a hard time understanding right away? Any known issues or bugs? Each repository should be laid out in a way that makes sense. For best practices on Python projects, see Cookiecutter. This is a package that will help you set up a coherent folder structure. File names should be all lowercase Package-level Packages should be documented in the module docstring of the init.py file in the package directory. Include in the init.py file a comment string like: &quot;&quot;&quot; This is the xyz package. &quot;&quot;&quot; Code-level Code-level documentation comes in the form of a document string, which is a string literal that occurs as the first statement in a module, function, class, or method definition. The docstring becomes the doc special attribute of that object. Classes, modules, and functions all use docustrings. Best practices for docstrings can be found in Python’s PEP 257 – Docstring Conventions guide. a. Summaries of this best practice can also be found in the Document Strings section of Python’s PEP 8 Style Guide. Rules of thumb ALWAYS use &quot;&quot;&quot;triple double quotes&quot;&quot;&quot; around docstrings ALWAYS include Inputs Outputs A summary of the object’s functionalities Example def new_func(x, y, z): &quot;&quot;&quot; The docstring indicates the purpose of the function :param x: a real number :type x: int :param y: a string :type y: str :param z: a boolean :type z: :returns: str --a concatenation of x, y, and z&quot;&quot;&quot; Environments Package and environment managers Most popular python tools for virtual environments: PIP: Python package manager; funnily enough, it stands for “Pip Installs Packages”“) Conda: Package and environment manager Img Source Conda Environment A conda environment is container that holds a specific version of the source software and packages. Setting Up a Conda Environment Open Git Bash Create a new environment by typing conda create --name NAME_OF_ENVIRONMENT. Adding python=3.7 or python=2.7 specificies which python to install. If No version is specified, the root version will be used. 3. Install the packages into the environment. Type conda install numpy. Package versions can be specified by adding numpy==1.11 ornumpy&gt;=1.11 Save the environment so others can replicate it. Type conda list -e &gt; requirements.txt. Version control the requirements file. Now others be able to build your exact environment, helping to mitigate python and package version issues Use an already created conda environment To see what environments exist type conda env list. The asterisk (*) indicates the current active environment Activate the environment of interest. source activate NAME_OF_ENVIRONMENT. Activating the environment allows access to the version of python and all the packages installed in that environment Copy an environment Using the requirements.text file type conda create --name NAME_OF_ENVIRONMENT --file requirements.txt Activate the environment by typing source activate NAME_OF_ENVIRONMENT Deleting an environment conda remove --name NAME_OF_ENVIRONMENT --all Package Development Folder Structure package_name\\ package_name\\ __init__.py Module1.py tests\\ module1_test.py .gitignore LICENSE README.md setup.py config.yaml The above folder shows how a simple python package can be setup. All python scripts should live in the package_name subfolder of the overall package. Tests live in a tests file which is nested in the package_name folder. LICENSE A file that specifies the type of license. This can just be written in plaintext. README.md The README for the package. Learn more about the README in the Documentation section of this book. setup.py Below is a sample setup.py file. The setup.py is a required package for python package development. It contains metadata about the package as well as some build information. from setuptools import setup with open(&quot;README.md&quot;, &quot;r&quot;) as fh: long_description = fh.read() setup( name=&#39;foo&#39;, version=&#39;1.0.0&#39;, author = &quot;Joli Holmes&quot;, author_email = &quot;holmesjoli@gmail.com&quot;, description=&#39;A useful module&#39;, long_description = long_description, url = &quot;https://github.com/holmesjoli/programmerKnowledgeBase&quot; packages= setuptools.find_packages(), install_requires=[] ) 5.2.0.1 config.yaml You may wish to include a config file in the package. See more in the config files section. Install locally packages Navigate to the package directory (the upper most package folder, which contains the setup.py file) Type pip install -e . Data Archiving Please use the dataArchivePy library. The dataArchivePy library is a library to help analysts link data and content. Workflow Develop code normally. Add and commit code to bitbucket/TFS and do pull requests. When it comes time to deliver content to a client/partner then use the dataArchivePy library. At this point the code should be code reviewed and tested and be ready to merge into master. Pull request development branch to master Checkout the master branch Use the Sample code and auto_commit modules and archive_files modules to commit and archive data and other content. 5.3 Quality Testing Rules of thumb Unit tests should have descriptive function names (often long!) Directory structure Set-up the directory this way. For each python module there should be an equivalent test_XXX.py or XXX_test.py module, to test the functions in the python module. dir\\ spam.py __init__.py tests\\ test_spam.py __init__.py Testing Libraries There are many different testing libraries in Python. This website here lists 40+. One of the easiest and most popular libraries to use for testing beginners is the unittest library. Unittest Python Module Example from Jeff Knupp: Script called primes.py. def is_prime(number): &quot;&quot;&quot;Return True if *number* is prime.&quot;&quot;&quot; for element in range(number): if number % element == 0: return False return True def print_next_prime(number): &quot;&quot;&quot;Print the closest prime number larger than *number*.&quot;&quot;&quot; index = number while True: index += 1 if is_prime(index): print(index) Python Test Module Import the module to test as the beginning of the script. If the functions to test are contained in an object then make an instance of the object and call the functions using dot notation. Script called test_primes.py. import unittest from primes import is_prime class PrimesTestCase(unittest.TestCase): &quot;&quot;&quot;Tests for `primes.py`.&quot;&quot;&quot; def test_is_five_prime(self): &quot;&quot;&quot;Is five successfully determined to be prime?&quot;&quot;&quot; self.assertTrue(is_prime(5)) if __name__ == &#39;__main__&#39;: unittest.main() Running the tests Navigate to the python module directory and run the following Run all the tests for all modules conda install pytest python -m pytest tests Run all tests for one module conda install pytest python -m pytest tests/test_spam.py Testing column types Test if column is an integer self.assertIsInstance(df.col, int) Test if column is a string self.assertIsInstance(df.col, str) Helpful Asserts to Know https://kapeli.com/cheat_sheets/Python_unittest_Assertions.docset/Contents/Resources/Documents/index 5.4 Data Management Import SAS import pandas as pd df = pd.read_sas(file, format = &quot;sas7bdat&quot;, encoding = &quot;iso-8859-1&quot;) Pandas 0.23.4+ works better Excel import pandas as pd xl = pd.ExcelFile(file) df = xl.parse(&#39;SHEET_NAME&#39;) CSV import pandas as pd df = pd.read_csv(file, encoding=&quot;ISO-8859-1&quot;) Export CSV import pandas as pd df.to_csv(&quot;path/name_of_file.csv&quot;) Excel import pandas as pd df.to_excel(&quot;df.xlsx&quot;, sheet_name = &quot;Sheet 1&quot;) Reshaping Wide to Long import pandas as pd df_long = pd.melt(df, id_vars = &#39;state&#39;, value_vars = cols) Stack Overflow Long to Wide import pandas as pd df_wide = df.pivot(index, columns, values) Index is the column of columns to keep in long format Columns are the columns that will go from long to wide Values are the values which will populate the columns Stack Overflow Converting column types Integer to string df[&quot;col_str&quot;] = df[&quot;col_int&quot;].apply(str) String to numeric import pandas as pd df[&quot;col_num&quot;] = pd.to_numeric(df[&quot;col_str&quot;]) String to datetime df[&quot;datetime&quot;] = pd.to_datetime(df[&quot;datetime&quot;], format=&quot;%m/%d/%Y %I:%M%p&quot;) df[&quot;date&quot;] = df[&quot;datetime&quot;].dt.date Other common data management functions Rename columns import pandas as pd df.rename(columns = {&quot;old_name&quot;: &quot;new_name&quot;}, inplace = True) # OR df = df.rename(columns = {&quot;old_name&quot;: &quot;new_name&quot;}) Drop duplicates import pandas as pd df.drop_duplicates(inplace = True) # OR df = df.drop_duplicates() If else import numpy as np df[&#39;monthly&#39;] = np.where((df[&#39;X&#39;] == 1) | (df[&#39;Y&#39;] == 1), 1, 0) 5.5 Command Line Interface (CLI) Python can be run from the command line python path_to_file/file.py 5.6 Web Web testing Install selenium with conda 1. Open Anaconda Prompt 2. `conda install selenium` Additional information about the install can be found in the Python Installation docs Launching a selenium driver The python distribution of selenium relies on downloadable webdrivers12. Chrome and Firefox drivers are preferred, though good web-testers will test across all platforms! Chrome Firefox Edge Internet Explorer Safari13 Test to see if selenium is set-up properly Open python and run the following from selenium import webdriver cpath = &#39;path_to_downloaded_and_unzipped_chrome_driver&#39; fpath = &#39;path_to_downloaded_and_unzipped_firefox_driver&#39; chrome_driver = webdriver.Chrome(executable_path = cpath) #OR firefox_driver = webdriver.Firefox(executable_path = fpath) A chrome driver and firefox driver should launch with the execution of these lines Version controlling drivers Like any software, drivers are updated and change over time, and the way in which the selenium package interacts with these drivers is subject to change over time. A good practice, especially if others will be utilizing the same code is to version control the drivers. Otherwise other users of the code will have to go download their own drivers, which could results in different driver versions and cause issues. Drivers can be added to TFS the same way any other file is added to git. Waits Explicit waits are better to use than implicit waits http://toolsqa.com/selenium-webdriver/implicit-explicit-n-fluent-wait/ Expected Conditions Waits title_is title_contains presence_of_element_located visibility_of_element_located visibility_of presence_of_all_elements_located text_to_be_present_in_element text_to_be_present_in_element_value frame_to_be_available_and_switch_to_it invisibility_of_element_located element_to_be_clickable staleness_of element_to_be_selected element_located_to_be_selected element_selection_state_to_be element_located_selection_state_to_be alert_is_present From https://selenium-python.readthedocs.io/waits.html Speed http://letztest.blogspot.com/2016/03/10-tips-for-improving-speed-of.html 5.7 Resources General Style Guides Pep 8 Reproducibility Environments Conda Environments Anaconda commands cheat sheet Why you need Python environments and how to manage them with Conda - Gergely Szerovay Have you installed a Python package, but now you can’t import it? - Thomas Kluyver Package Development Packaging Python Projects Minimal Package Structure Packaging and Testing Packaging and distributing projects Quality Testing Improve Your Python: Understanding Unit Testing - Jeff Knupp Testing Your Code - The Hitchhikers Guide to Python Beyond Unit Tests: Taking Your Testing to the Next Level - Hillel Wayne What the mock?—A cheatsheet for mocking in Python - Yeray Diaz Let there be tests unittest Data Management Cheatsheat If the webdriver download in a zipped file, be sure to upzip the file↩ Cannot be downloaded on a Windows computer↩ "],
["r-1.html", "6 R 6.1 Introduction 6.2 Reproducibility 6.3 Quality 6.4 Data Management 6.5 Modeling 6.6 Command Line Interface (CLI) 6.7 Resources", " 6 R 6.1 Introduction R is used primarily for data cleaning and modeling. R is also used to build simple applications and html files using Rmarkdown, Rshiny and Rshiny dashboards. R can also be used with for Bayesian analyses. Installation Download R 3.5.2 Download Rstudio14 Download Rtools35 Package Repositories R’s main package repository is CRAN (the Comprehensive R Archive Network). However, like most open source-code, many people and organizations maintain code on Github. The devtools package allows uses to download packages directly from github. Initial workflow Follow these initial first steps to set-up your R-project. Start a new project Open a terminal which can read Rscript calls Navigate to the folder where R packages are stored Clone the ProjectTemplateR package Run the following line the terminal, replacing ProjName with the name of your project Rscript ProjectTemplateR/R/main.R -p &quot;ProjName&quot; Version control files .gitignore .Rbuildignore .Rprofile .travis.yaml DESCRIPTION NAMESPACE LICENSE README.md XXX.Rproj packrat/ init.R packrat.lock packrat.opts R/ tests/ man/* Debugging R offers a number of useful built-in functions for debugging, and user-contributed packages are also available to provide additional functionality. The simplest method is to insert commands into your code to print diagnostic“check points” during its execution, using functions such as cat(), print(), or sink(). R also has a more sophisticated debugging system, which is supported by a GUI in the RStudio IDE. The most direct way to mark a function for debugging is to use debug(functionname). When this function is called, normal execution will be suspended, and you will enter the browser display. Commands within the debugged function can then be executed one at a time, and output can be inspected by the programmer along the way. You can set a function to begin debugging partway through its execution with setBreakpoint() or trace(). If an error is encountered (whether or not the function was being debugged), the traceback() command can be used to view the stack of nestedfunction calls at the time of the most recent error. We illustrate debugging in RStudio with the following example. Suppose we want to debug the lm() function to see how it works inside: debug(lm) # mark lm() for debugging lm(speed ~ dist(data=cars)) A few things happen when you execute the debugged lm() function: In the Source pane, a window called the Source Viewer opens. This contains the code of lm(). At the bottom of the Environment pane, the current stack of function calls is listed under Traceback. The console enters Browser mode. In browser mode, a set of buttons appears along the top of the console (including Next, Continue, Stop, and two arrow buttons), and the console prompt becomes Browse[2]&gt;. The current location in lm() is indicated by the green arrow on the left side of the Source Viewer. In the above figure, the green arrow is on line 4, after the function declaration, but before any of the commands within the function have been executed. This means that all the variables that were entered (or defaulted) in the function call are available. We can enter commands at the command prompt as usual, and they will be executed in the environment of the function: The commands of the lm() function are executed by clicking the button, by typing n+Enter, or by pressing Enter in the console. Any of these actions will execute the next command in the lm() code, and advance the green arrow in the Source Viewer to the next command. The other control options are: At any point in executing the lm() code, you can run your own commands to look at the contents of an object created within lm(), for example, or check objects for errors. When you are finished debugging, exit debugging mode and then run undebug(lm), so the function can be used normally without debugging. More information on debugging in RStudio can be found here, or in the chapter on debugging in the Advanced R book. External packages such as debug and edtdbg contain alternative debugging interfaces, which are not described here. 6.2 Reproducibility Config files YAML Below is example code of how to read in and write out yaml files. The same functions, documented and tested, can be found in the utilsR package. Example file ## You can add comments to YAML! VAR1: 1 VAR2: 2 Import library(yaml) config &lt;- yaml.load_file(&quot;./config.yaml&quot;) var1 &lt;- config$VAR1 var2 &lt;- config$VAR2 Note: Avoid warnings by adding a final line without any spaces or indents in it! Update library(yaml) config$var3 &lt;- &quot;VAR3&quot; yml &lt;- yaml::as.yaml(config) write(yml, &quot;./config.yaml&quot;) JSON Below is example code of how to read in and write out json files. The same functions, documented and tested, can be found in the utilsR package. Example file { &quot;VAR1&quot; = 1, &quot;VAR2&quot; = 2 } Import library(rjson) config &lt;- rjson::fromJSON(file = &quot;./config.json&quot;) Update library(rjson) library(jsonlite) json &lt;- rjson::toJSON(config) jsonlist::write_json(config, file = &quot;./config.json&quot;) CSV Example file |VAR1 | VAR2| |-----|-----| | 1 | 2 | Import config &lt;- read.csv(&quot;./config.csv&quot;) #OR library(readr) config &lt;- read_csv(&quot;./config.csv&quot;) var1 &lt;- config$VAR1 var2 &lt;- config$VAR2 Update config$var3 &lt;- &quot;VAR3&quot; write.csv(config, &quot;./config.csv&quot;, row.names = FALSE) Documentation Documenting Functions Document all functions with roxygen comments. Roxygen is a Hadley Wickham package which makes the process of function documentation much easier. Roxygen comments, added to each function, can automatically be updated in the .Rd files. Programmers only have to keep documentation up-to-date in one place and the rest is automated. Roxygen Comments Example roxygen comments below: func_y &lt;- function(x, y) { #&#39; Short title #&#39; #&#39; Longer description, e.g. this functions adds x and y #&#39; #&#39; @param x the integer x #&#39; @param y the integer y #&#39; @return what the function returns #&#39; @examples #&#39; func_y(x, y) return(x + y) } It’s important to keep these comments up-to-date, because running devtools::document() will copy what exists from the roxygen comments into package documentation. Each function should be documented with three tags at minimum: @param, @examples and @return @param appears as many times in the documentation as there are parameters in the function @examples appears once and documents an example of how the function can be used @return appears once and documents what the function returns Vignettes A vignette is a long-form guide to a package. Like a book chapter or academic paper that describes the problem that your package is designed to solve and then shows the reader how to solve it. Adopt a beginners mindset, and teach the vignette in person to get feedback. install.packages(&quot;rmarkdown&quot;) usethis::use_vignette(&quot;vignette_name&quot;) This will automatically create a vignettes folder Environments There are several different methods to use environments with R: Packrat Anaconda the appDependencies() function from RSConnect checkpoint This section will primarily focus on Packrat, which is maintained by RStudio. However, if you’ve read the [pyenvironment] section and found that method preferable, the same method can be applied to R. Packrat Packrat is one way R users have developed to manage packages and R versions. Packrat is:15 Isolated: Installing a new or updated package for one project won’t break your other projects, and vice versa. That’s because packrat gives each project its own private package library. Portable: Easily transport your projects from one computer to another, even across different platforms. Packrat makes it easy to install the packages your project depends on. Reproducible: Packrat records the exact package versions you depend on, and ensures those exact versions are the ones that get installed wherever you go. Steps Initialize packrat Current project devtools::install_github(&quot;rstudio/packrat&quot;) packrat::init(infer.dependencies = F) Note: packrat can take a really long, sometimes frustrating long time to set-up all the packages. It’s highly suggested to set-up the environment before starting a project and getting into the weeds. New package While building your new package be sure to check the option highlighted in red: Save snapshot Once all the necessary packages have been downloaded save a snapshot of the packages. Periodically update the snapshot when new dependencies are added. packrat::snapshot() The snapshot will add the packages, versions, and package sources the the packrat.lock file. Update environment If you add packages after the initial snapshot is taken, then call the status and then take a new snapshot. packrat::status() packrat::snapshot() Share package libraries There are two ways to share environments. One is through bunding and sending the bundle to a collaborator. The more preferred way is to use git to version control several files. Git Version control these files: packrat/init.R packrat/packrat.lock packrat/packrat.opts .Rprofile This is the preferred method of sharing environments because everything can be stored in a version controlled repository and easily re-created. Bundling Run packrat::bundle() Share the bundle with collaborators who have R. Collaborators will have to run packrat::unbundle() to access the code Recreate the environment packrat::restore() Where are my project packages? Packrat downloads the project-packages into several locations Base R packages: Packrat/lib-R/32_or_64_bit_r/version_of_r Other packages: Packrat/lib/32_or_64_bit_r/version_of_r Ignoring local repositories Unfortunately, packrat will throw an error for local packages which cannot be found on CRAN. Ignore local packages packrat::opts$ignored.packages(c(&quot;package1&quot;, &quot;package2&quot;)) Package Development If you’re going to be developing packages, I would strongly suggest reading Hadley Wickham’s R-packages. It’s a great place to start learning how to develop a package. General Guidelines NEVER use library(foo) or require(foo) Code in a package is only executed when the package is built. It doesn’t get re-executed when the package is loaded, so the code won’t work when a user of the package loads a function that requires foo Put dependencies in the DEPENDENCIES file of the R package NEVER use source(&quot;script.R&quot;) The source call modifies the current environment Instead use devtools::load_all() When relying on other packages, refer to the these through package::function() Documentation Add Roxygen comments to add functions devtools::document() will take Roxygen comments and turn them into .Rd files An Rd file is a documentation for a single R function that can be accessed from the console using ?function_name Install local package install.packages(&quot;path_to_package&quot;, repos = NULL, type = &quot;source&quot;) Update local package detach(&quot;package:package_name&quot;, unload=TRUE) install.packages(&quot;path_to_package&quot;, repos = NULL, type = &quot;source&quot;) Data Archiving Please use the dataArchiveR library. The dataArchiveR library is a library to help analysts link data and content. Workflow Develop code normally. Add and commit code to bitbucket/TFS and do pull requests. When it comes time to deliver content to a client/partner then use the dataArchiveR library. At this point the code should be code reviewed and tested and be ready to merge into master. Pull request development branch to master Checkout the master branch Use the Sample code to get started. Also, checkout the README and vignette for more information about the package and how the dataArchiveR packge can be integrated into a pipeline. 6.3 Quality Testing The most popular testing library in R is probably Hadley Wickham’s testthat package. Run the following code to automatically generate the tests/testthat directory. Test files like in the tests/testthat directory. usethis::use_testthat() The name of the file must start with test, e.g. test_package.R Tests are organized hierarchically, expectations are grouping into tests which are organized into files Tests should have descriptive names, so that the message associated with the test helps you to quickly identify the problem Code Style Style Guides Google Style Guide Hadley Style Guide16 General Rules Variable names and function names should be lowercase or camel case (be consistent) Avoid using names of existing functions/variables Spacing Put spaces around all infix operators (=, +, -, &lt;-, etc.) ALWAYS put a space after a comma, and never before Put a space before left parentheses, except in a function call Curly Braces Opening curly brace { NEVER goes on its own line, but should always be followed by a new line ALWAYS indent code inside curly braces Closing curly brace } always goes on its own line, unless followed by an else Line Length Roughly 80 - 90 characters per line Indentation ALWAYS use two spaces NEVER mix tabs/spaces Assignment ALWAYS &lt;- not = Comments Comments should begin with # Comments should explain the why not the what Comments that are too lengthy indicate a function should probably be used to break up the code chunk Styling Packages formatR Automatically formats and spaces code for more human-readability. library(formatR) formatR::tidy_dir(&quot;R&quot;) styler Goal is to provide non-invasive pretty-printing of R source code while adhering to the tidyverse formatting rules. library(styler) ugly_code &lt;- &quot;a=function( x){1+1} style_text(ugly_code) # style_file() # style_dir() # style_pkg() lintr Warns you about styling errors and potential problems. library(lintr) lintr::lint(test.R) 6.4 Data Management Packages Package Description and Usage dplyr Tools for manipulating data frame reshape2 Tools to reshape data frames tidyr Tools for creating “tidy” datasets data.table Enhanced data.frame for faster operations lubridate Tools to parse and manipulate dates stringr Tools to manipulate character strings Matrix Sparse and dense matrix classes abind Combines multi-dimenstional arrays 6.5 Modeling Packages Package Description and Usage survey Analysis of complex survey samples twang Toolkit for weighting and analysis of nonequivalent groups Matching Multivariate and propensity score matching with balance forecast Forecasting functions for time series and linear models boot Functions for bootstrap analysis multcomp Tools for multiple comparison testing rstan R interface to Stan for Bayesian analysis rjags Just another Gibbs sampler in R survival Survival analysis nnet Neural networks and multinomial log-linear models lme4 Linear and non-linear mixed effects models nlme Linear and non-linear mixed effects models mgcv Generalized additive models refund Regression with functional data spatial Functions for kriging and point pattern analysis KernSmooth Functions for kernel smoothing lars Least angle regression, lasso, and forward stagewise glmnet Lasso and elastic-net regression methods caret Classification and regression training rpart Recursive partitioning for classification and regression trees randomForest Random forest methods from machine learning cluster Cluster analysis 6.6 Command Line Interface (CLI) 6.6.1 Running R Scripts R can be run from the command line. Rscript path_to_R_script.R 6.6.2 Adding Command Line Arguments 6.7 Resources General Advanced R - Hadley Wickham Swirl Courses Swirl Courses Git Repo Quick R - Rob Kabacoff R Studio Cheatsheets - Rstudio Style Guides R Style Guide - Hadley Wickham Google Style Guide - Google Data Management Data wrangling cheatsheet - Rstudio Reproducibility Packaging data analytical work reproducibly using R (and friends) - Marwick, Boettiger, Mullen Package Development R Packages - Hadley Wickham R extensions manual goodrpackages - Maelle Salmon Remote packages - Hadley Wickham and Jim Hester Environments Creating Reproducible Software Environments with Packrat - Brian Connelly Modeling Data Science R for Data Science - Hadley Wickham Visualization ggplot2 - Hadley Wickham R Graphics Cookbook - Winston Chang ggplot2 cheatsheet - Rstudio Quality Testing Getting Started with Testthat - Hadley Wickham R packages - Hadley Wickham Command Line Interface Passing arguments to an R script from command lines Efficiency Parallelization Quick Intro to Parallel Computing in R - Matt Jones Multicode Data Science with R and Python Beyond Single-Core R - Jonathan Dursi Note: download R before downloading Rstudio↩ Text from↩ This is based off of the Google Style Guide↩ "],
["stata-1.html", "7 Stata 7.1 Resources", " 7 Stata Stata is primarily used for statistical analysis and visualization. It is one of the most popular statistical softwares in Economics. Unlike R and Python, Stata is not open-source. The the majority of Stata packages are maintained by Stata, although some users do build packages for Stata as well. There are several pros and cons because of this. One con is that, Stata isn’t free like R and Python. Another con is that the online community isn’t as active as R and Python communities. However, Stata maintains lots of online help and resources for their users. 7.1 Resources General Stata video tutorials UCLA data analysis examples "],
["data-visualization.html", "8 Data Visualization 8.1 Tableau", " 8 Data Visualization 8.1 Tableau Introduction Tableau is a data visualization software. It’s a very popular tool for data and business analysts because it’s easy to learn and doesn’t require the developer of the tool programming background. It has powerful capabilities for creating dashboards and story boards which can be used for exploratory analysis or to lead end users through a particular story in the data. Pros and Cons Anyone who has created a dashboard before knows that it takes a long time, even a simple one using software like Rshiny. A complex one using Javascript’s visualization library, D3, takes even longer. The other issue with creating a custom dashboard in a software like R or JavaScript, is that the dashboard has to be maintained by a programmer who knows the language(s) used to build the dashboard. So, there are a lot of reasons to use Tableau. However, as a risk-averse programmer there are several things I would NEVER use Tableau for, even though Tableau has the capability to do so. The most important one is data management. A lot of the examples and use cases of Tableau use very clean, ready-to-use data, or the data needs to have one or two very simple changes, such as splitting the column of changing the column type. In my world, the data are never clean like that. They almost always have to be a) validated and b) managed. Although, there are some capabilities that allow a user to clean data in Tableau, I would strongly advise against that. There isn’t an easy way to track changes made to the data. As programmers, we use version control to track all our changes and then submit pull requests to have those changes approved. I would recommend maintaining this workflow for quality assurance purposes. Beginner Resources Tableau tips for beginners Video Powerpoint Tableau Developer Advanced Tableau Github Tableau Developer Conference Materials Tableau Versions Like any software or application, Tableau has changed and been updated overtime. However, specific functionalities, especially development functionalities, such as integration with R, Python, and Matlab and use of the Extract API depend on the version of Tableau install. Make sure that development you’re interested in doing is compatible with the version of Tableau that you have installed.17 To check the version go to Help &gt; About Tableau. Recommended Workflow Although, Tableau does not require a programming background, much of what we’ll do does require a lot of programming. Write data validation checks in Python Utilize all the tools discussed in Section 1 to write robust, quality code Write data management steps in Python Utilize all the tools discussed in Section 1 to write robust, quality code Use Tableau Extract API to create data extract Load data extract in Tableau Train model in Python and save to server Integrate trained model into Tableau Tableau Extract API The Tableau Extract API is for … Set-up Download the Tableau Extract API Extract the contents of the zip file to your repositories folder, and rename the folder tableausdk Create and activate new python environment conda create --name Tableau python==3.7 source activate Tableau Build the library from the command line python setup.py build python setup.py install Write the python script Resources Leveraging the Extract API to build sophisticated data models Video Powerpoint Code Integration with Statistical Software Tableau can integrate with both Python and R, such that Python or R code can be evaluated from within a Tableau workbook. This allows for the use of a number of machine learning packages and model predictions to be executed inside Tableau. Instead of writing an R or Python script outside of Tableau, running the model, exporting the results as data, importing those data back into Tableau, everything can be done in one software, which simplifies things significantly! Tableau relies on two packages the Rserve package for R-users and the TabPy package for python-users. Tableau Script Functions There are 4 script function in Tableau, which are used to return vectors from R/Python of specific types. SCRIPT_REAL() Returns real, or decimal, numbers SCRIPT_INT() Returns integer, or whole, numbers SCRIPT_STR() Returns strings (words and text) SCRIPT_BOOL() Returns booleans (true/false) Helpful tips All input vectors and the output vector must be of the same length External service connections can only return one vector at a time [Parameters] need some special treatment: .argn[1] instead of .argn Calculation scope is defined by the addressing &amp; partitioning settings of the Table Calculation Resources Accelerate Your Advanced Analytics R, Python &amp; MATLAB Video Powerpoint Data Science Applications with TabPy/R Video Powerpoint R Connect R and Tableau In R Install the Rserve package install.packages(&quot;Rserve&quot;) In the R console type Rserve::Rserve(port = 6311) In Tableau Connect Tableau to R Server Choose R settings Test the connection to makes sure it says “successfully connected to the external service” Workflow Source Resources Using R and Tableau Whitepaper Integrate R and Tableau Tutorial Using R within Tableau Tutorial R and Tableau Data Science Speed Python Connect Python and Tableau In IDE Clone the TabPy repo Install the TabPy packages pip install ./tabpy-server pip install ./tabpy-tools Install python 3.6.5 into the environment conda install python==3.6.5 Navigate to the Tabpy repository and type ./startup.cmd to initiate the server In Tableau Connect Tableau to Python Server Choose Python settings In a browser type localhost::9004 and the Tableau logo should appear Test the connection to makes sure it says “successfully connected to the external service” Starter Script Python import pandas as pd def loanclassifierfull(_arg1, _arg2): &quot;&quot;&quot; &quot;&quot;&quot; d = {&quot;1-grade&quot;: _arg1, &quot;2-grade&quot;: _arg2} df = pd.DataFrame(data = d) probs = model.predict_proba(df) return [loan[1] for loan in probs] client = tabpy_client.Client(&quot;http://localhost:9004&quot;) client.deploy(&quot;loanclassifierfull&quot;, loanclassifierfull, &quot;Description of the function&quot;) Tableau SCRIPT_REAL(&quot;return tabpy.query(&#39;loancliassifierfull&#39;, _arg1, _arg2)[&#39;response&#39;]&quot;, ([Arg1]), ([Arg2])) Resources Automating production with Python and Tableau APIs Video Powerpoint Building Data Science Applications in Tableau Video Powerpoint Additional Resources Leveraging the power of Python Blog Building advanced analytics applications Blog Forecasting with Python and Tableau Glossary Term Definition A Annotations Text boxes used to call out a specific mark point or an entire area in the view. Mark annotation stays with the mark D Data blend Combining data from multiple data sources into one view. Sends separate queries to the different data sources and then aggregates the data in Tableau. Data source A connection to a database or other place where data are stored. Dimension A dimension is a field that is independent. By default, Tableau treats any field containing qualitative, categorical information as a dimension. Drilling down Expanding out a hierarchy; ‘drilling down’ to the next level of detail. E Extract A snapshot of the data (.tde or .hyper file) L Live connection A data source that contains a direct connection to underlying data which provides real-time or near real-time data M Measure A variable that is a dependnet variable. Tableau treats any field containing numeric information as a measure. P Parameter Provide a single output to be used by another element. Only useful when the other element is in use. Parameters support a variety of data types. Pil A pill refers to a measure or dimension. It derives it’s name from the pill shape in the Tableau UI. T TDE A .tde file is a Tableau Data Extract File. A TDE is a compressed snapshot of the data stored on disk and loaded into memory as required to render a Tableau visualization. Tableau changed their version naming convention in 2018. Older versions are named 4.0, or 5.0, or 10.1. These go all the way from 4.0 to 10.5. In 2018, they started naming their versions after the year they released the version in. The lastest versions are named 2018.2, 2019.1, etc.↩ "],
["technical-terminology.html", "Technical Terminology", " Technical Terminology Term Abbreviation Definition B Backwards compatibility Packages that get updated are still compatible with code developed from an earlier version of that package. Binary package A distribution methods to a developer who doesn’t have package development tools. Also, a single file. Bundled package A package that’s been compressed into a single file C Central processing unit CPU Primary component of a computer that processes instructions. Contains at least one processor which is the actual chip inside the CPU which performs calculations. For many years, most CPUs had only one processor, but now it is common for a single CPU to have at least two processors of “processing cores” Compiled A program called a compiler, takes the plaintext source file and produces a native binary executable that the computer knows how to run intrinsically. When this happens it is called compile-time, and is distinct from run-time, which is when and executable is run and the program actually does things. Coverage Code coverage is a percentage which indicates the amount of code (lines, functions, and branches) in the project that is tested. It’s an indicator of how well maintained and stable the project is.18 D Dynamically typed Variable types are not explicitly declared. Among dynamically typed languages are Python, R, and JavaScript. E Extract, transform, load ETL the pipeline of data from it’s raw state into a clean, useable state. G Global type inference Types are inferred from context. Explicit type annotations are instead for the benefit of the programmers, and serve as machine-verified documentation. I In memory package A package which has been loaded into memory and then attaches it to the search path Installed package Binary package that’s been decompressed into a package library Integrated Development Environment IDE An IDE is a software editor that provides additional facilities to developers that aren’t available through traditional text editors (E.g. Notepad). Interpreted Interpreted languages have programs called interpreters that read plaintext source files and execute the instructions at the same moment. No native binary executable is created. Languages like Python and R are interpreted. L Lazily evaluated Values are not computed until required. This allows for functions to compose together in a performant way (no more work than necessary will be done), and allows for the use of infinite data structures (you could define the list of all prime numbers and only those you actually use will be computed). However, it also makes it more difficult to reason about performance, because it is sometimes not obvious when or if a particular computation will be performed. P Parallel processing A type of processing where multiple tasks are completed at a time by different processors. Processor See Central processing unit Purely functional Every function is a function in the mathematical sense (IE pure). No squinting required. Rather than functions having side-effects, are first-class values governed by the type system. To put it another way, code has only expressions that are evaluated; it does not have statements that are executed. R Representation State Transfer REST Defines a set of rules for messages sent from the client to be interpretted by the server. S Serial A process in which one task is completed at a given point in time and all the tasks are run by a processor in sequence. The alternative is parallel processing. Also known as sequential processing. Sequential processing See Serial processing. Single-threaded Utilization of a single processor. Source package A directory with all the components. Statically typed Every expression has a type which is determined at compile-time. If the types of any expressions don’t match up appropriately, the compiler will reject your program – there are guaranteed to be no type errors at run-time. Statically typed languages include C#, C++, TypeScript, and Java. T Test-driven development TDD Software development process that requires developers to write tests for code, before writing the actual code itself.19 https://www.gregjs.com/javascript/2016/how-to-get-a-shiny-coverage-badge-for-your-github-project/↩ https://medium.freecodecamp.org/test-driven-development-what-it-is-and-what-it-is-not-41fa6bca02a2↩ "]
]
